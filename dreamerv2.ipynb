{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNATEflF02x8jclKB59UU3m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6d4bd8efa7544e5aa6898187267868b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_686eb089700142b985508f7f7d7636a0",
              "IPY_MODEL_1a84f507e14e4f4c817568ff32471186"
            ],
            "layout": "IPY_MODEL_460405f2498f4d66b46ab2592c7588f4"
          }
        },
        "686eb089700142b985508f7f7d7636a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c547d09a6e804c08970e74f4858dd14e",
            "placeholder": "​",
            "style": "IPY_MODEL_324ea17a4b1b4341b84f0e543377c901",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "1a84f507e14e4f4c817568ff32471186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9266207a1c254c7db96f5dff5999c5fd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69b6c973f5d64717af052c5498b8729f",
            "value": 1
          }
        },
        "460405f2498f4d66b46ab2592c7588f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c547d09a6e804c08970e74f4858dd14e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "324ea17a4b1b4341b84f0e543377c901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9266207a1c254c7db96f5dff5999c5fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69b6c973f5d64717af052c5498b8729f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd8136ab23294ded8cf635c4b1c6a538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_973e4d052af44b82b05790d2c58d090b",
              "IPY_MODEL_194831df6a974c8b86623abe7f4c18ca",
              "IPY_MODEL_67ebe9c515b247f2a98b30583ed37406"
            ],
            "layout": "IPY_MODEL_7b85a868d134402b87b8fa1e76636f89"
          }
        },
        "973e4d052af44b82b05790d2c58d090b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb9a53be4d4493687b020671cd8a830",
            "placeholder": "​",
            "style": "IPY_MODEL_d6e02ad43d034c8b97b779b1e6eec771",
            "value": "  0%"
          }
        },
        "194831df6a974c8b86623abe7f4c18ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1283821ea98a4ac58c611866101c7d77",
            "max": 10000000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c8a15d7b58a45ec8beb682823d093e2",
            "value": 13311
          }
        },
        "67ebe9c515b247f2a98b30583ed37406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e0ee76151dc435185bc32a326cf8d54",
            "placeholder": "​",
            "style": "IPY_MODEL_80f89c98dc4243fea2102011dd9d5bd5",
            "value": " 13311/10000000 [2:44:58&lt;2092:43:19,  1.33it/s]"
          }
        },
        "7b85a868d134402b87b8fa1e76636f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eb9a53be4d4493687b020671cd8a830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6e02ad43d034c8b97b779b1e6eec771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1283821ea98a4ac58c611866101c7d77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c8a15d7b58a45ec8beb682823d093e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e0ee76151dc435185bc32a326cf8d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80f89c98dc4243fea2102011dd9d5bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiraiG/my_world_model/blob/main/dreamerv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ライブラリのimport"
      ],
      "metadata": {
        "id": "htM9jW1UyiJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
        "!pip install gym==0.21.0 gym[atari]==0.21.0 gym[accept-rom-license]==0.21.0 autorom ale-py\n",
        "!pip install ale-py\n",
        "!pip install -q -U einops datasets\n",
        "!pip install wandb\n",
        "!pip install pybullet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4DSHl65M8807",
        "outputId": "dba47889-2fae-4826-8cf5-5267ebd4e51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting setuptools==65.5.0\n",
            "  Downloading setuptools-65.5.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wheel<0.40.0\n",
            "  Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: wheel, setuptools\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.43.0\n",
            "    Uninstalling wheel-0.43.0:\n",
            "      Successfully uninstalled wheel-0.43.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.3 requires setuptools>65.5.1, but you have setuptools 65.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-65.5.0 wheel-0.38.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              },
              "id": "0d45338d759a40d4b7bce0b2aff84edc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting autorom\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Collecting ale-py\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.21.0) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.21.0) (2.2.1)\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2 (from gym==0.21.0)\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom) (2.31.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym==0.21.0) (4.66.2)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gym==0.21.0)\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2024.2.2)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616797 sha256=532b480eb1c3e3aed86a1d5dd12d839cbe50627d208552d9ba4cabf634885336\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/aa/90/b67df76370d3916a2189b662cf48da38ce41a4e7e58b6abff5\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=b88d57a426a474e808419ae52485db8271b77ea8cc5d47e6be45b05fe39ebcdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: gym, ale-py, AutoROM.accept-rom-license, autorom\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.7.5 autorom-0.4.2 gym-0.21.0\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.7.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.25.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py) (6.4.0)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m796.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.16.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.44.0-py2.py3-none-any.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.9/264.9 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (65.5.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.44.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.5\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# 3d203c250261c0fa5b739811895ac4752a70e3b0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "vPmVommH6Ua2",
        "outputId": "d8a20d3b-fc18-4e17-d583-4c079d04c2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGrMf09QNTH8",
        "outputId": "37b24671-771d-4a32-e904-de7b300db74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 31 09:28:35 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKf2RwW7xGWG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import gym\n",
        "from gym.wrappers import ResizeObservation\n",
        "import pybullet_envs\n",
        "\n",
        "import torch\n",
        "import torch.distributions as td\n",
        "from torch.distributions import Normal, Categorical, OneHotCategorical, OneHotCategoricalStraightThrough\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm.notebook import tqdm\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RSSM"
      ],
      "metadata": {
        "id": "I0LPlGqByoHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim, max_len=1000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        # Compute positional encodings\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encodings to input\n",
        "        return x + self.pe[:, :x.size(1)].detach()\n"
      ],
      "metadata": {
        "id": "H0ug36NRSQRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RSSM(nn.Module):\n",
        "    def __init__(self, mlp_hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int, actino_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_hidden_dim = rnn_hidden_dim\n",
        "        self.state_dim = state_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Recurrent model\n",
        "        # h_t = f(h_t-1, z_t-1, a_t-1)\n",
        "        self.transition_hidden = nn.Linear(state_dim * num_classes + action_dim, mlp_hidden_dim)\n",
        "        self.transition = nn.GRUCell(mlp_hidden_dim, rnn_hidden_dim)\n",
        "\n",
        "        # transition predictor\n",
        "        self.prior_hidden = nn.Linear(rnn_hidden_dim, mlp_hidden_dim)\n",
        "        self.prior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n",
        "\n",
        "        # representation model\n",
        "        self.posterior_hidden = nn.Linear(rnn_hidden_dim + 1536, mlp_hidden_dim)\n",
        "        self.posterior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n",
        "\n",
        "    def recurrent(self, state: torch.Tensor, action: torch.Tensor, rnn_hidden: torch.Tensor):\n",
        "        # recullent model: h_t = f(h_t-1, z_t-1, a_t-1)を計算する\n",
        "        hidden = F.elu(self.transition_hidden(torch.cat([state, action], dim=1)))\n",
        "        rnn_hidden = self.transition(hidden, rnn_hidden)\n",
        "\n",
        "        return rnn_hidden  # h_t\n",
        "\n",
        "    def get_prior(self, rnn_hidden: torch.Tensor, detach: bool = False):\n",
        "        # transition predictor: \\hat{z}_t ~ p(z\\hat{z}_t | h_t)\n",
        "        hidden = F.elu(self.prior_hidden(rnn_hidden))\n",
        "        logits = self.prior_logits(hidden)\n",
        "        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n",
        "        if detach:\n",
        "            logits = logits.detach()\n",
        "\n",
        "        prior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "        return prior_dist  # p(z\\hat{z}_t | h_t)\n",
        "\n",
        "    def get_posterior(self, rnn_hidden: torch.Tensor, embedded_obs: torch.Tensor, detach: bool = False):\n",
        "        # representation predictor: z_t ~ q(z_t | h_t, o_t)\n",
        "        hidden = F.elu(self.posterior_hidden(torch.cat([rnn_hidden, embedded_obs], dim=1)))\n",
        "        logits = self.posterior_logits(hidden)\n",
        "        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n",
        "        if detach:\n",
        "            logits = logits.detach()\n",
        "\n",
        "        posterior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "        return posterior_dist  # q(z_t | h_t, o_t)"
      ],
      "metadata": {
        "id": "wywbO6pISMDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class RSSM(nn.Module):\n",
        "#     def __init__(self, mlp_hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int, actino_dim: int):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.rnn_hidden_dim = rnn_hidden_dim\n",
        "#         self.state_dim = state_dim\n",
        "#         self.num_classes = num_classes\n",
        "\n",
        "#         # Recurrent model\n",
        "#         # h_t = f(h_t-1, z_t-1, a_t-1)\n",
        "#         self.transition_hidden = nn.Linear(state_dim * num_classes + action_dim, mlp_hidden_dim)\n",
        "#         self.transition = nn.GRUCell(mlp_hidden_dim, rnn_hidden_dim)\n",
        "\n",
        "#         # transition predictor\n",
        "#         self.prior_hidden = nn.Linear(rnn_hidden_dim, mlp_hidden_dim)\n",
        "#         self.prior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n",
        "\n",
        "#         # representation model\n",
        "#         self.posterior_hidden = nn.Linear(rnn_hidden_dim + 1536, mlp_hidden_dim)\n",
        "#         self.posterior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n",
        "\n",
        "#     def recurrent(self, state: torch.Tensor, action: torch.Tensor, rnn_hidden: torch.Tensor):\n",
        "#         # recullent model: h_t = f(h_t-1, z_t-1, a_t-1)を計算する\n",
        "#         hidden = F.elu(self.transition_hidden(torch.cat([state, action], dim=1)))\n",
        "#         rnn_hidden = self.transition(hidden, rnn_hidden)\n",
        "\n",
        "#         return rnn_hidden  # h_t\n",
        "\n",
        "#     def get_prior(self, rnn_hidden: torch.Tensor, detach: bool = False):\n",
        "#         # transition predictor: \\hat{z}_t ~ p(z\\hat{z}_t | h_t)\n",
        "#         hidden = F.elu(self.prior_hidden(rnn_hidden))\n",
        "#         logits = self.prior_logits(hidden)\n",
        "#         logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n",
        "#         if detach:\n",
        "#             logits = logits.detach()\n",
        "\n",
        "#         prior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "#         return prior_dist  # p(z\\hat{z}_t | h_t)\n",
        "\n",
        "#     def get_posterior(self, rnn_hidden: torch.Tensor, embedded_obs: torch.Tensor, detach: bool = False):\n",
        "#         # representation predictor: z_t ~ q(z_t | h_t, o_t)\n",
        "#         hidden = F.elu(self.posterior_hidden(torch.cat([rnn_hidden, embedded_obs], dim=1)))\n",
        "#         logits = self.posterior_logits(hidden)\n",
        "#         logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n",
        "#         if detach:\n",
        "#             logits = logits.detach()\n",
        "\n",
        "#         posterior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "#         return posterior_dist  # q(z_t | h_t, o_t)"
      ],
      "metadata": {
        "id": "UuXoAbFmxNof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## エンコーダの実装"
      ],
      "metadata": {
        "id": "Ad3b3c6KyqYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### attention"
      ],
      "metadata": {
        "id": "Ehp6FVODjY7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32, num_mem_kv=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, dim, 1),\n",
        "            nn.GroupNorm(1, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x ( b, dim, h', w' )\n",
        "        Returns:\n",
        "            out ( b, dim, h', w' )\n",
        "        \"\"\"\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q = rearrange(q, \"b (h c) x y -> b h c (x y)\", h=self.heads)\n",
        "        k = rearrange(k, \"b (h c) x y -> b h c (x y)\", h=self.heads)\n",
        "        v = rearrange(v, \"b (h c) x y -> b h c (x y)\", h=self.heads)\n",
        "        # ( b, heads, dim_head, h(=x) * w(=y) )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "\n",
        "        return self.to_out(out)"
      ],
      "metadata": {
        "id": "Dkp_s_jDygZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### encoder"
      ],
      "metadata": {
        "id": "BD7n2PBDjawp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 48, kernel_size=4, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(48)\n",
        "        self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(96)\n",
        "        self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(192)\n",
        "        self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n",
        "        self.bn4 = nn.BatchNorm2d(384)\n",
        "\n",
        "        self.attention = LinearAttention(384, heads=4, dim_head=32)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor):\n",
        "        \"\"\"\n",
        "        観測画像をベクトルに埋め込むためのEncoder．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        obs : torch.Tensor (B, C, H, W)\n",
        "            入力となる観測画像．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        embedded_obs : torch.Tensor (B, D)\n",
        "            観測画像をベクトルに変換したもの．Dは入力画像の幅と高さに依存して変わる．\n",
        "            入力が(B, 3, 64, 64)の場合，出力は(B, 1536)になる．\n",
        "        \"\"\"\n",
        "        x = F.elu(self.conv1(obs))\n",
        "        x = F.elu(self.conv2(x))\n",
        "        x = F.elu(self.conv3(x))\n",
        "        x = self.bn4(self.conv4(x))\n",
        "        # print(\"attention適用前 : \", x.shape)\n",
        "        embedded_obs = self.attention(x).view(x.size(0), -1)\n",
        "        # embedded_obs = x.view(x.size(0), -1)\n",
        "        # print(\"attention適用後 : \", embedded_obs.shape)\n",
        "\n",
        "        return embedded_obs  # x_t"
      ],
      "metadata": {
        "id": "lRHifwGXyY_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## デコーダーの定義"
      ],
      "metadata": {
        "id": "0us1YFvD5ey_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(state_dim*num_classes + rnn_hidden_dim, 1536)\n",
        "        self.dc1 = nn.ConvTranspose2d(1536, 192, kernel_size=5, stride=2)\n",
        "        self.dc2 = nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2)\n",
        "        self.dc3 = nn.ConvTranspose2d(96, 48, kernel_size=6, stride=2)\n",
        "        self.dc4 = nn.ConvTranspose2d(48, 1, kernel_size=6, stride=2)\n",
        "\n",
        "\n",
        "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n",
        "        \"\"\"\n",
        "        決定論的状態と，確率的状態を入力として，観測画像を復元するDecoder．\n",
        "        出力は多次元正規分布の平均値をとる．\n",
        "\n",
        "        Paremters\n",
        "        ---------\n",
        "        state : torch.Tensor (B, state_dim * num_classes)\n",
        "            確率的状態．\n",
        "        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
        "            決定論的状態．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        obs_dist : torch.distribution.Independent\n",
        "            観測画像を再構成するための多次元正規分布．\n",
        "        \"\"\"\n",
        "        hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n",
        "        hidden = hidden.view(hidden.size(0), 1536, 1, 1)\n",
        "        hidden = F.elu(self.dc1(hidden))\n",
        "        hidden = F.elu(self.dc2(hidden))\n",
        "        hidden = F.elu(self.dc3(hidden))\n",
        "        mean = self.dc4(hidden)\n",
        "\n",
        "        obs_dist = td.Independent(td.Normal(mean, 1), 3)\n",
        "        return obs_dist  # p(\\hat{x}_t | h_t, z_t)"
      ],
      "metadata": {
        "id": "ndR57zfcz6Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.conv1 = nn.Conv2d(1, 48, kernel_size=4, stride=2)\n",
        "#         self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n",
        "#         self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n",
        "#         self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n",
        "\n",
        "#     def forward(self, obs: torch.Tensor):\n",
        "#         \"\"\"\n",
        "#         観測画像をベクトルに埋め込むためのEncoder．\n",
        "\n",
        "#         Parameters\n",
        "#         ----------\n",
        "#         obs : torch.Tensor (B, C, H, W)\n",
        "#             入力となる観測画像．\n",
        "\n",
        "#         Returns\n",
        "#         -------\n",
        "#         embedded_obs : torch.Tensor (B, D)\n",
        "#             観測画像をベクトルに変換したもの．Dは入力画像の幅と高さに依存して変わる．\n",
        "#             入力が(B, 3, 64, 64)の場合，出力は(B, 1536)になる．\n",
        "#         \"\"\"\n",
        "#         hidden = F.elu(self.conv1(obs))\n",
        "#         hidden = F.elu(self.conv2(hidden))\n",
        "#         hidden = F.elu(self.conv3(hidden))\n",
        "#         embedded_obs = self.conv4(hidden).reshape(hidden.size(0), -1)\n",
        "\n",
        "#         return embedded_obs  # x_t\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "#         super().__init__()\n",
        "#         self.fc = nn.Linear(state_dim*num_classes + rnn_hidden_dim, 1536)\n",
        "#         self.dc1 = nn.ConvTranspose2d(1536, 192, kernel_size=5, stride=2)\n",
        "#         self.dc2 = nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2)\n",
        "#         self.dc3 = nn.ConvTranspose2d(96, 48, kernel_size=6, stride=2)\n",
        "#         self.dc4 = nn.ConvTranspose2d(48, 1, kernel_size=6, stride=2)\n",
        "\n",
        "\n",
        "#     def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n",
        "#         \"\"\"\n",
        "#         決定論的状態と，確率的状態を入力として，観測画像を復元するDecoder．\n",
        "#         出力は多次元正規分布の平均値をとる．\n",
        "\n",
        "#         Paremters\n",
        "#         ---------\n",
        "#         state : torch.Tensor (B, state_dim * num_classes)\n",
        "#             確率的状態．\n",
        "#         rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
        "#             決定論的状態．\n",
        "\n",
        "#         Returns\n",
        "#         -------\n",
        "#         obs_dist : torch.distribution.Independent\n",
        "#             観測画像を再構成するための多次元正規分布．\n",
        "#         \"\"\"\n",
        "#         hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n",
        "#         hidden = hidden.view(hidden.size(0), 1536, 1, 1)\n",
        "#         hidden = F.elu(self.dc1(hidden))\n",
        "#         hidden = F.elu(self.dc2(hidden))\n",
        "#         hidden = F.elu(self.dc3(hidden))\n",
        "#         mean = self.dc4(hidden)\n",
        "\n",
        "#         obs_dist = td.Independent(td.Normal(mean, 1), 3)\n",
        "#         return obs_dist  # p(\\hat{x}_t | h_t, z_t)"
      ],
      "metadata": {
        "id": "4AQ4gviNLgSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 報酬関数"
      ],
      "metadata": {
        "id": "gHuNtc4A5n94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n",
        "        \"\"\"\n",
        "        決定論的状態と，確率的状態を入力として，報酬を予測するモデル．\n",
        "        出力は正規分布の平均値をとる．\n",
        "\n",
        "        Paremters\n",
        "        ---------\n",
        "        state : torch.Tensor (B, state_dim * num_classes)\n",
        "            確率的状態．\n",
        "        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
        "            決定論的状態．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        reward_dist : torch.distribution.Independent\n",
        "            報酬を予測するための正規分布．\n",
        "        \"\"\"\n",
        "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = F.elu(self.fc2(hidden))\n",
        "        hidden = F.elu(self.fc3(hidden))\n",
        "        mean = self.fc4(hidden)\n",
        "\n",
        "        reward_dist = td.Independent(td.Normal(mean, 1),  1)\n",
        "        return reward_dist  # p(\\hat{r}_t | h_t, z_t)"
      ],
      "metadata": {
        "id": "_ld0BJgt5nW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscountModel(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n",
        "        \"\"\"\n",
        "        決定論的状態と，確率的状態を入力として，現在の状態がエピソード終端かどうか判別するモデル．\n",
        "        出力はベルヌーイ分布の平均値をとる．\n",
        "\n",
        "        Paremters\n",
        "        ---------\n",
        "        state : torch.Tensor (B, state_dim * num_classes)\n",
        "            確率的状態．\n",
        "        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
        "            決定論的状態．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        discount_dist : torch.distribution.Independent\n",
        "            状態が終端かどうかを予測するためのベルヌーイ分布．\n",
        "        \"\"\"\n",
        "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = F.elu(self.fc2(hidden))\n",
        "        hidden = F.elu(self.fc3(hidden))\n",
        "        mean= self.fc4(hidden)\n",
        "\n",
        "        discount_dist = td.Independent(td.Bernoulli(logits=mean),  1)\n",
        "        return discount_dist  # p(\\hat{\\gamma}_t | h_t, z_t)"
      ],
      "metadata": {
        "id": "wRU5EsY67opn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor-Critic"
      ],
      "metadata": {
        "id": "JyuiIYah8L90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, action_dim: int, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor, eval: bool = False):\n",
        "        \"\"\"\n",
        "        確率的状態を入力として，criticで推定される価値が最大となる行動を出力する．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : torch.Tensor (B, state_dim * num_classes)\n",
        "            確率的状態．\n",
        "        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
        "            決定論的状態．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action : torch.Tensor (B, 1)\n",
        "            行動．\n",
        "        action_log_prob : torch.Tensor(B, 1)\n",
        "            予測した行動をとる確率の対数．\n",
        "        action_entropy : torch.Tensor(B, 1)\n",
        "            予測した確率分布のエントロピー．エントロピー正則化に使用．\n",
        "        \"\"\"\n",
        "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = F.elu(self.fc2(hidden))\n",
        "        hidden = F.elu(self.fc3(hidden))\n",
        "        hidden = F.elu(self.fc4(hidden))\n",
        "        logits = self.out(hidden)\n",
        "\n",
        "        if eval:\n",
        "            action = torch.argmax(logits, dim=1)\n",
        "            action = F.one_hot(action, logits.shape[1])\n",
        "            return action, None, None\n",
        "\n",
        "        action_dist = OneHotCategorical(logits=logits)  # 行動をサンプリングする分布: p_{\\psi} (\\hat{a}_t | \\hat{z}_t)\n",
        "        action = action_dist.sample()  # 行動: \\hat{a}_t\n",
        "\n",
        "        # Straight-Throught Estimatorで勾配を通す．\n",
        "        action = action + (action_dist.probs - action_dist.probs.detach())\n",
        "\n",
        "        action_log_prob = action_dist.log_prob(torch.round(action.detach()))\n",
        "        action_entropy = action_dist.entropy()\n",
        "\n",
        "        return action, action_log_prob, action_entropy"
      ],
      "metadata": {
        "id": "3k7RPQvq8M7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor):\n",
        "        \"\"\"\n",
        "        確率的状態を入力として，価値関数(lambda target)の値を予測する．．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : torch.Tensor (B, state_dim * num_classes)\n",
        "            確率的状態．\n",
        "        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
        "            決定論的状態．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        value : torch.Tensor (B, 1)\n",
        "            入力された状態に対する状態価値関数の予測値．\n",
        "        \"\"\"\n",
        "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = F.elu(self.fc2(hidden))\n",
        "        hidden = F.elu(self.fc3(hidden))\n",
        "        hidden = F.elu(self.fc4(hidden))\n",
        "        value = self.out(hidden)\n",
        "\n",
        "        return value  # v_{\\xi}(\\hat{z}_t)の近似（論文に合わせて決定論的）"
      ],
      "metadata": {
        "id": "MPFpQs7a8P3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## リプレイバッファ"
      ],
      "metadata": {
        "id": "OTzsanbX8bZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    \"\"\"\n",
        "    RNNを用いて訓練するのに適したリプレイバッファ\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity, observation_shape, action_dim):\n",
        "        self.capacity = capacity\n",
        "\n",
        "        self.observations = np.zeros((capacity, *observation_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n",
        "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
        "        self.done = np.zeros((capacity, 1), dtype=bool)\n",
        "\n",
        "        self.index = 0\n",
        "        self.is_filled = False\n",
        "\n",
        "    def push(self, observation, action, reward, done):\n",
        "        \"\"\"\n",
        "        リプレイバッファに経験を追加する\n",
        "        \"\"\"\n",
        "        self.observations[self.index] = observation\n",
        "        self.actions[self.index] = action\n",
        "        self.rewards[self.index] = reward\n",
        "        self.done[self.index] = done\n",
        "\n",
        "        # indexは巡回し, 最も古い経験を上書きする\n",
        "        if self.index == self.capacity - 1:\n",
        "            self.is_filled = True\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, chunk_length):\n",
        "        \"\"\"\n",
        "        経験をリプレイバッファからサンプルします. （ほぼ）一様なサンプルです\n",
        "        結果として返ってくるのは観測(画像), 行動, 報酬, 終了シグナルについての(batch_size, chunk_length, 各要素の次元)の配列です\n",
        "        各バッチは連続した経験になっています\n",
        "        注意: chunk_lengthをあまり大きな値にすると問題が発生する場合があります\n",
        "        \"\"\"\n",
        "        episode_borders = np.where(self.done)[0]\n",
        "        sampled_indexes = []\n",
        "        for _ in range(batch_size):\n",
        "            cross_border = True\n",
        "            while cross_border:\n",
        "                initial_index = np.random.randint(len(self) - chunk_length + 1)\n",
        "                final_index = initial_index + chunk_length - 1\n",
        "                cross_border = np.logical_and(initial_index <= episode_borders,\n",
        "                                              episode_borders < final_index).any()#論理積\n",
        "            sampled_indexes += list(range(initial_index, final_index + 1))\n",
        "\n",
        "        sampled_observations = self.observations[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, *self.observations.shape[1:])\n",
        "        sampled_actions = self.actions[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, self.actions.shape[1])\n",
        "        sampled_rewards = self.rewards[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, 1)\n",
        "        sampled_done = self.done[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, 1)\n",
        "        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.is_filled else self.index"
      ],
      "metadata": {
        "id": "sbiRZe_h8QnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ターゲット関数の計算"
      ],
      "metadata": {
        "id": "iLRW-Ny28hET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_lambda_target(rewards: torch.Tensor, discounts: torch.Tensor, values: torch.Tensor, lambda_: float):\n",
        "    \"\"\"\n",
        "    lambda targetを計算する関数．\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    rewards : torch.Tensor (imagination_horizon, D)\n",
        "        報酬．1次元目が時刻tを表しており，2次元目は自由な次元数にでき，想像の軌道を作成するときに入力されるサンプルindexと考える．\n",
        "    discounts : torch.Tensor (imagination_horizon, D)\n",
        "        割引率．gammaそのままを利用するのではなく，DiscountModelの出力をかけて利用する．\n",
        "    values : torch.Tensor (imagination_horizon, D)\n",
        "        状態価値関数．criticで予測された値を利用するが，Dreamer v2ではtarget networkで計算する．\n",
        "    lambda_ : float\n",
        "        lambda targetのハイパラ．\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    V_lambda : torch.Tensor (imagination_horizon, D)\n",
        "        lambda targetの値．\n",
        "    \"\"\"\n",
        "    V_lambda = torch.zeros_like(rewards)\n",
        "\n",
        "    for t in reversed(range(rewards.shape[0])):\n",
        "        if t == rewards.shape[0] - 1:\n",
        "            V_lambda[t] = rewards[t] + discounts[t] * values[t]  # t=Hの場合（式4の下の条件）\n",
        "        else:\n",
        "            V_lambda[t] = rewards[t] + discounts[t] * ((1-lambda_) * values[t+1] + lambda_ * V_lambda[t+1])\n",
        "\n",
        "    return V_lambda"
      ],
      "metadata": {
        "id": "GT7cEVjS8eVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_obs(obs):\n",
        "    \"\"\"\n",
        "    画像の変換. [0, 255] -> [0, 1]\n",
        "    \"\"\"\n",
        "    height, width = obs.shape[0], obs.shape[1]\n",
        "    obs = Image.fromarray(obs)\n",
        "    obs = obs.convert(\"L\")\n",
        "    obs = np.array(obs).reshape(height, width, 1)\n",
        "    obs = obs.astype(np.float32)\n",
        "    normalized_obs = obs / 255.0 - 0.5\n",
        "    return normalized_obs"
      ],
      "metadata": {
        "id": "lds45XO88kod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    ActionModelに基づき行動を決定する. そのためにRSSMを用いて状態表現をリアルタイムで推論して維持するクラス\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, rssm, action_model):\n",
        "        self.encoder = encoder\n",
        "        self.rssm = rssm\n",
        "        self.action_model = action_model\n",
        "\n",
        "        self.device = next(self.action_model.parameters()).device\n",
        "        self.rnn_hidden = torch.zeros(1, rssm.rnn_hidden_dim, device=self.device)\n",
        "\n",
        "    def __call__(self, obs, eval=False):\n",
        "        # preprocessを適用, PyTorchのためにChannel-Firstに変換\n",
        "        obs = preprocess_obs(obs)\n",
        "        obs = torch.as_tensor(obs, device=self.device)\n",
        "        obs = obs.transpose(1, 2).transpose(0, 1).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 観測を低次元の表現に変換し, posteriorからのサンプルをActionModelに入力して行動を決定する\n",
        "            embedded_obs = self.encoder(obs)\n",
        "            state_posterior = self.rssm.get_posterior(self.rnn_hidden, embedded_obs)\n",
        "            state = state_posterior.sample().flatten(1)\n",
        "            action, _, _  = self.action_model(state, self.rnn_hidden, eval=eval)\n",
        "\n",
        "            # 次のステップのためにRNNの隠れ状態を更新しておく\n",
        "            self.rnn_hidden = self.rssm.recurrent(state, action, self.rnn_hidden)\n",
        "\n",
        "        return action.squeeze().cpu().numpy()\n",
        "\n",
        "    #RNNの隠れ状態をリセット\n",
        "    def reset(self):\n",
        "        self.rnn_hidden = torch.zeros(1, self.rssm.rnn_hidden_dim, device=self.device)"
      ],
      "metadata": {
        "id": "o7rZ5w4V8nrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習の準備"
      ],
      "metadata": {
        "id": "8TJsmWoE8rWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        # data settings\n",
        "        self.buffer_size = 2_000_000  # バッファにためるデータの上限\n",
        "        self.batch_size = 50  # 学習時のバッチサイズ\n",
        "        self.seq_length = 50  # 各バッチの系列長\n",
        "        self.imagination_horizon = 15  # 想像上の軌道の系列長\n",
        "\n",
        "        # model dimensions\n",
        "        self.state_dim = 32  # 確率的な状態の次元数\n",
        "        self.num_classes = 32  # 確率的な状態のクラス数（離散表現のため）\n",
        "        self.rnn_hidden_dim = 600  # 決定論的な状態の次元数\n",
        "        self.mlp_hidden_dim = 400  # MLPの隠れ層の次元数\n",
        "\n",
        "        # learning params\n",
        "        self.model_lr = 2e-4  # world model(trainsition / prior / posterior / discount / image predictor)の学習率\n",
        "        self.actor_lr = 4e-5  # actorの学習率\n",
        "        self.critic_lr = 1e-4  # criticの学習率\n",
        "        self.epsilon = 1e-5  # optimizerのepsilonの値\n",
        "        self.weight_decay = 1e-6  # weight decayの係数\n",
        "        self.gradient_clipping = 100  # 勾配クリッピング\n",
        "        self.kl_scale = 0.1  # kl lossのスケーリング係数\n",
        "        self.kl_balance = 0.8  # kl balancingの係数(fix posterior)\n",
        "        self.actor_entropy_scale = 1e-3  # entropy正則化のスケーリング係数\n",
        "        self.slow_critic_update = 100  # target critic networkの更新頻度\n",
        "        self.reward_loss_scale = 1.0  # reward lossのスケーリング係数\n",
        "        self.discount_loss_scale = 5.0  # discount lossのスケーリング係数\n",
        "\n",
        "        # lambda return params\n",
        "        self.discount = 0.99  # 割引率\n",
        "        self.lambda_ = 0.95  # lambda returnのパラメータ\n",
        "\n",
        "        # learning piriod settings\n",
        "        self.num_iter = 10_000_000  # イテレーション数\n",
        "        self.seed_iter = 50_000  # 事前にランダム行動で探索する回数\n",
        "        self.update_freq = 4  # パラメータを更新する頻度\n",
        "        self.eval_freq = 10  # 評価頻度（エピソード）\n",
        "        self.eval_episodes = 5  # 評価に用いるエピソード数\n",
        "\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "eDDfg3Q18pTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RepeatAction(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    同じ行動を指定された回数自動的に繰り返すラッパー. 観測は最後の行動に対応するものになる\n",
        "    \"\"\"\n",
        "    def __init__(self, env, skip=4):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.height = env.observation_space.shape[0]\n",
        "        self.width = env.observation_space.shape[1]\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info"
      ],
      "metadata": {
        "id": "CtX6vLXl-frq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 環境"
      ],
      "metadata": {
        "id": "PmlmGolXJBOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(seed=1234, img_size=64):\n",
        "    env = gym.make(\"ALE/MontezumaRevenge-v5\")\n",
        "    # env = gym.make('HalfCheetahBulletEnv-v0')\n",
        "\n",
        "    # シード固定\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    # np.random.seed(seed)\n",
        "\n",
        "    # Dreamerでは観測は64x64のRGB画像\n",
        "    env = ResizeObservation(env, (img_size, img_size))\n",
        "    env = RepeatAction(env, skip=4)\n",
        "\n",
        "    return env"
      ],
      "metadata": {
        "id": "ZesAboxv-c88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int) -> None:\n",
        "    \"\"\"\n",
        "    Pytorch, NumPyのシード値を固定します．これによりモデル学習の再現性を担保できます．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : int\n",
        "        シード値．\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "gRniU24C-g_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルパラメータをGoogleDriveに保存・後で読み込みするためのヘルパークラス\n",
        "class TrainedModels:\n",
        "    def __init__(self, *models) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        models : nn.Module\n",
        "            保存するモデル．複数モデルを渡すことができます．\n",
        "\n",
        "        使用例: trained_models = TraindModels(encoder, rssm, value_model, action_model)\n",
        "        \"\"\"\n",
        "        assert np.all([nn.Module in model.__class__.__bases__ for model in models]), \"Arguments for TrainedModels need to be nn models.\"\n",
        "\n",
        "        self.models = models\n",
        "\n",
        "    def save(self, dir: str) -> None:\n",
        "        \"\"\"\n",
        "        initで渡したモデルのパラメータを保存します．\n",
        "        パラメータのファイル名は01.pt, 02.pt, ... のように連番になっています．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir : str\n",
        "            パラメータの保存先．\n",
        "        \"\"\"\n",
        "        for i, model in enumerate(self.models):\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(dir, f\"{str(i + 1).zfill(2)}.pt\")\n",
        "            )\n",
        "\n",
        "    def load(self, dir: str, device: str) -> None:\n",
        "        \"\"\"\n",
        "        initで渡したモデルのパラメータを読み込みます．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir : str\n",
        "            パラメータの保存先．\n",
        "        device : str\n",
        "            モデルをどのデバイス(CPU or GPU)に載せるかの設定．\n",
        "        \"\"\"\n",
        "        for i, model in enumerate(self.models):\n",
        "            model.load_state_dict(\n",
        "                torch.load(\n",
        "                    os.path.join(dir, f\"{str(i + 1).zfill(2)}.pt\"),\n",
        "                    map_location=device\n",
        "                )\n",
        "            )"
      ],
      "metadata": {
        "id": "QiNJXo1B-sEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(env: RepeatAction, policy: Agent, step: int, cfg: Config):\n",
        "    \"\"\"\n",
        "    評価用の関数．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: RepeatAction\n",
        "        環境のインスタンス．\n",
        "    policy : Agent\n",
        "        エージェントのインスタンス．\n",
        "    step : int\n",
        "        現状の訓練のステップ数．\n",
        "    cfg : Config\n",
        "        コンフィグ．\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    max_ep_rewards : float\n",
        "        評価中に1エピソードで得た最大の報酬和．\n",
        "    \"\"\"\n",
        "    all_ep_rewards = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(cfg.eval_episodes):\n",
        "            obs = env.reset()  # 環境をリセット\n",
        "            policy.reset()  # RNNの隠れ状態をリセット\n",
        "            done = False  # 終端条件\n",
        "            episode_reward = 0  # エピソードでの報酬和\n",
        "            while not done:\n",
        "                action = policy(obs, eval=True)\n",
        "                action_int = np.argmax(action)\n",
        "\n",
        "                obs, reward, done, _ = env.step(action_int)\n",
        "                episode_reward += reward\n",
        "\n",
        "            all_ep_rewards.append(episode_reward)\n",
        "\n",
        "        mean_ep_rewards = np.mean(all_ep_rewards)\n",
        "        max_ep_rewards = np.max(all_ep_rewards)\n",
        "        print(f\"Eval(iter={step}) mean: {mean_ep_rewards:.4f} max: {max_ep_rewards:.4f}\")\n",
        "\n",
        "    return max_ep_rewards"
      ],
      "metadata": {
        "id": "qUsqIA_C-uxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習"
      ],
      "metadata": {
        "id": "KxiM0qr_AtvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル等の初期化\n",
        "set_seed(1234)\n",
        "env = make_env()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "action_dim = env.action_space.n\n",
        "# action_dim = env.action_space.shape[0]\n",
        "# リプレイバッファ\n",
        "replay_buffer = ReplayBuffer(\n",
        "    capacity=cfg.buffer_size,\n",
        "    observation_shape=(64, 64, 1),\n",
        "    action_dim=env.action_space.n\n",
        "    # action_dim = env.action_space.shape[0]\n",
        ")\n",
        "\n",
        "# モデル\n",
        "rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "reward_model =  RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "discount_model = DiscountModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "target_critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "trained_models = TrainedModels(\n",
        "    rssm,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    reward_model,\n",
        "    discount_model,\n",
        "    actor,\n",
        "    critic\n",
        ")\n",
        "\n",
        "# optimizer\n",
        "wm_params = (list(rssm.parameters())         +\n",
        "             list(encoder.parameters())      +\n",
        "             list(decoder.parameters())      +\n",
        "             list(reward_model.parameters()) +\n",
        "             list(discount_model.parameters())\n",
        "            )\n",
        "wm_optimizer = torch.optim.AdamW(wm_params, lr=cfg.model_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
        "actor_optimizer = torch.optim.AdamW(actor.parameters(), lr=cfg.actor_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
        "critic_optimizer = torch.optim.AdamW(critic.parameters(), lr=cfg.critic_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)"
      ],
      "metadata": {
        "id": "-aUOLJAw8vlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ランダム行動でバッファを埋める\n",
        "\n",
        "# pbar = tqdm(total=cfg.seed_iter, unit='buffer size')\n",
        "while len(replay_buffer) < cfg.seed_iter:\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        next_obs, reward, done, _ = env.step(action)\n",
        "        action = F.one_hot(torch.tensor(action), num_classes=env.action_space.n)\n",
        "        # action = F.one_hot(torch.tensor(action), num_classes=env.action_space.shape[0])\n",
        "        replay_buffer.push(preprocess_obs(obs), action, np.tanh(reward), done)\n",
        "        obs = next_obs\n",
        "    # pbar.update(len(replay_buffer))\n",
        "\n",
        "# episodes = 5\n",
        "# for episode in tqdm(range(episodes), total = episodes):\n",
        "#     obs = env.reset()\n",
        "#     done = False\n",
        "#     while not done:\n",
        "#         action = env.action_space.sample()\n",
        "#         next_obs, reward, done, _ = env.step(action)\n",
        "#         replay_buffer.push(obs, action, reward, done)\n",
        "#         obs = next_obs\n",
        "\n",
        "# episodes = 5\n",
        "# for episode in tqdm(range(episodes), total=episodes):\n",
        "#     obs = env.reset()\n",
        "#     done = False\n",
        "#     while not done:\n",
        "#         action = env.action_space.sample()  # ランダムな行動をサンプリング\n",
        "#         next_obs, reward, done, _ = env.step(action)\n",
        "#         # 行動をone-hotベクトルに変換\n",
        "#         action = int((action - env.action_space.low) / (env.action_space.high - env.action_space.low) * (env.action_space.shape[0] - 1))\n",
        "#         replay_buffer.push(preprocess_obs(obs), action, np.tanh(reward), done)  # 前処理した観測、行動、報酬、終了フラグをバッファに追加\n",
        "#         obs = next_obs\n",
        "# pbar.close()"
      ],
      "metadata": {
        "id": "brUBUhjw-xfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習の開始"
      ],
      "metadata": {
        "id": "qub7al_gQxW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習を行う\n",
        "# 環境と相互作用 → 一定イテレーションでモデル更新を繰り返す\n",
        "policy = Agent(encoder, rssm, actor)\n",
        "\n",
        "# 環境，収益等の初期化\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "total_episode = 1\n",
        "best_reward = 0\n",
        "\n",
        "# wandbの初期化\n",
        "wandb.init(project=\"強化学習_最終課題\")\n",
        "wandb.config.update(cfg)\n",
        "\n",
        "for iteration in tqdm(range(cfg.num_iter)):\n",
        "    with torch.no_grad():\n",
        "        # 環境と相互作用\n",
        "        action = policy(obs)  # モデルで行動をサンプリング(one-hot)\n",
        "        action_int = np.argmax(action)  # 環境に渡すときはint型\n",
        "        next_obs, reward, done, _ = env.step(action_int)  # 環境を進める\n",
        "\n",
        "        # 得たデータをリプレイバッファに追加して更新\n",
        "        replay_buffer.push(preprocess_obs(obs), action, np.tanh(reward), done)  # x_t, a_t, r_t, gamma_t\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "\n",
        "    if (iteration + 1) % cfg.update_freq == 0:\n",
        "        # モデルの学習\n",
        "        # リプレイバッファからデータをサンプリングする\n",
        "        # (batch size, seq_lenght, *data shape)\n",
        "        observations, actions, rewards, done_flags =\\\n",
        "            replay_buffer.sample(cfg.batch_size, cfg.seq_length)\n",
        "        done_flags = 1 - done_flags  # 終端でない場合に1をとる\n",
        "\n",
        "        # torchで扱える形（seq lengthを最初の次元に，画像はchnnelを最初の次元にする）に変形，observationの前処理\n",
        "        observations = torch.permute(torch.as_tensor(observations, device=device), (1, 0, 4, 2, 3))  # (T, B, C, H, W)\n",
        "        actions = torch.as_tensor(actions, device=device).transpose(0, 1)  # (T, B, action dim)\n",
        "        rewards = torch.as_tensor(rewards, device=device).transpose(0, 1)  # (T, B, 1)\n",
        "        done_flags = torch.as_tensor(done_flags, device=device).transpose(0, 1).float()  # (T, B, 1)\n",
        "\n",
        "        # =================\n",
        "        # world modelの学習\n",
        "        # =================\n",
        "        # 観測をベクトルに埋めこみ\n",
        "        emb_observations = encoder(observations.reshape(-1, 1, 64, 64)).view(cfg.seq_length, cfg.batch_size, -1)  # (T, B, 1536)\n",
        "\n",
        "        # 状態表現z，行動aはゼロで初期化\n",
        "        # バッファから取り出したデータをt={1, ..., seq length}とするなら，以下はz_1とみなせる\n",
        "        state = torch.zeros(cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n",
        "        rnn_hidden = torch.zeros(cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n",
        "\n",
        "        # 各観測に対して状態表現を計算\n",
        "        # タイムステップごとに計算するため，先に格納するTensorを定義する(t={1, ..., seq length})\n",
        "        states = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n",
        "        rnn_hiddens = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n",
        "\n",
        "        # prior, posteriorを計算してKL lossを計算する\n",
        "        kl_loss = 0\n",
        "        for i in range(cfg.seq_length-1):\n",
        "            # rnn hiddenを更新\n",
        "            rnn_hidden = rssm.recurrent(state, actions[i], rnn_hidden)  # h_t+1\n",
        "\n",
        "            # prior, posteriorを計算\n",
        "            next_state_prior = rssm.get_prior(rnn_hidden) # \\hat{z}_t+1\n",
        "            next_state_posterior = rssm.get_posterior(rnn_hidden, emb_observations[i+1])  # z_t+1\n",
        "\n",
        "            # posteriorからzをサンプリング\n",
        "            state = next_state_posterior.rsample().flatten(1)\n",
        "            rnn_hiddens[i+1] = rnn_hidden  # h_t+1\n",
        "            states[i+1] = state  # z_t+1\n",
        "\n",
        "            # KL lossを計算\n",
        "            kl_loss += (\n",
        "                        (cfg.kl_balance * kl_divergence(rssm.get_posterior(rnn_hidden, emb_observations[i+1], detach=True), next_state_prior) + \\\n",
        "                        (1 - cfg.kl_balance) * kl_divergence(next_state_posterior, rssm.get_prior(rnn_hidden, detach=True)))\n",
        "                       ).mean()\n",
        "        kl_loss /= (cfg.seq_length - 1)\n",
        "\n",
        "        # 初期状態は使わない\n",
        "        rnn_hiddens = rnn_hiddens[1:]  # (seq lenghth - 1, batch size rnn hidden)\n",
        "        states = states[1:]  # (seq length - 1, batch size, state dim * num_classes)\n",
        "\n",
        "        # 得られた状態を利用して再構成，報酬，終端フラグを予測\n",
        "        # そのままでは時間方向，バッチ方向で次元が多いため平坦化\n",
        "        flatten_rnn_hiddens = rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((T-1) * B, rnn hidden)\n",
        "        flatten_states = states.view(-1, cfg.state_dim * cfg.num_classes)  # ((T-1) * B, state_dim * num_classes)\n",
        "\n",
        "        # 上から再構成，報酬，終端フラグ予測\n",
        "        obs_dist = decoder(flatten_states, flatten_rnn_hiddens)  # (T * B, 3, 64, 64)\n",
        "        reward_dist = reward_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n",
        "        discount_dist = discount_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n",
        "\n",
        "        # 各予測に対する損失の計算（対数尤度）\n",
        "        C, H, W = observations.shape[2:]\n",
        "        obs_loss = -obs_dist.log_prob(observations[1:].reshape(-1, C, H, W)).mean()\n",
        "        reward_loss = -reward_dist.log_prob(rewards[:-1].reshape(-1, 1)).mean()\n",
        "        discount_loss = -discount_dist.log_prob(done_flags[:-1].reshape(-1, 1)).mean()\n",
        "\n",
        "        # 総和をとってモデルを更新\n",
        "        wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.discount_loss_scale * discount_loss + cfg.kl_scale * kl_loss\n",
        "\n",
        "        wm_optimizer.zero_grad()\n",
        "        wm_loss.backward()\n",
        "        clip_grad_norm_(wm_params, cfg.gradient_clipping)\n",
        "        wm_optimizer.step()\n",
        "\n",
        "        #====================\n",
        "        # Actor, Criticの更新\n",
        "        #===================\n",
        "        # wmから得た状態の勾配を切っておく\n",
        "        flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n",
        "        flatten_states = flatten_states.detach()\n",
        "\n",
        "        # priorを用いた状態予測\n",
        "        # 格納する空のTensorを用意\n",
        "        imagined_states = torch.zeros(cfg.imagination_horizon + 1,\n",
        "                                      *flatten_states.shape,\n",
        "                                      device=flatten_states.device)\n",
        "        imagined_rnn_hiddens = torch.zeros(cfg.imagination_horizon + 1,\n",
        "                                           *flatten_rnn_hiddens.shape,\n",
        "                                           device=flatten_rnn_hiddens.device)\n",
        "        imagined_action_log_probs = torch.zeros((cfg.imagination_horizon + 1, cfg.batch_size * (cfg.seq_length-1)),\n",
        "                                                device=flatten_rnn_hiddens.device)\n",
        "        imagined_action_entropys = torch.zeros((cfg.imagination_horizon + 1, cfg.batch_size * (cfg.seq_length-1)),\n",
        "                                                device=flatten_rnn_hiddens.device)\n",
        "\n",
        "        #　未来予測をして想像上の軌道を作る前に, 最初の状態としては先ほどモデルの更新で使っていた\n",
        "        # リプレイバッファからサンプルされた観測データを取り込んだ上で推論した状態表現を使う\n",
        "        imagined_states[0] = flatten_states\n",
        "        imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n",
        "\n",
        "        # open-loopで予測\n",
        "        for i in range(1, cfg.imagination_horizon + 1):\n",
        "            actions, action_log_probs, action_entropys = actor(flatten_states, flatten_rnn_hiddens)  # ((T-1) * B, action dim)\n",
        "\n",
        "            # rnn hiddenを更新, priorで次の状態を予測\n",
        "            rnn_hidden = rssm.recurrent(flatten_states, actions, flatten_rnn_hiddens)  # h_t+1\n",
        "            flatten_states_prior = rssm.get_prior(flatten_rnn_hiddens)\n",
        "            flatten_states = flatten_states_prior.rsample().flatten(1)\n",
        "\n",
        "            imagined_rnn_hiddens[i] = flatten_rnn_hiddens\n",
        "            imagined_states[i] = flatten_states\n",
        "            imagined_action_log_probs[i-1] = action_log_probs\n",
        "            imagined_action_entropys[i-1] = action_entropys\n",
        "\n",
        "        actions, action_log_probs, action_entropys = actor(flatten_states, flatten_rnn_hiddens)  # ((T-1) * B, action dim)\n",
        "        imagined_action_log_probs[-1] = action_log_probs\n",
        "        imagined_action_entropys[-1] = action_entropys\n",
        "\n",
        "        # 得られた状態から報酬を予測\n",
        "        flatten_imagined_states = imagined_states.view(-1, cfg.state_dim * cfg.num_classes)  # ((imagination horizon + 1) * (T-1) * B, state dim * num classes)\n",
        "        flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((imagination horizon + 1) * (T-1) * B, rnn hidden)\n",
        "\n",
        "        # reward, done_flagsは分布なので平均値をとる\n",
        "        # ((imagination horizon + 1), (T-1) * B)\n",
        "        imagined_rewards = reward_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).mean.view(cfg.imagination_horizon + 1, -1)\n",
        "        imagined_values = critic(flatten_imagined_states, flatten_imagined_rnn_hiddens).view(cfg.imagination_horizon + 1, -1)\n",
        "        target_values = target_critic(flatten_imagined_states, flatten_imagined_rnn_hiddens).view(cfg.imagination_horizon + 1, -1)\n",
        "        imagined_done_flags = discount_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).base_dist.probs.view(cfg.imagination_horizon + 1, -1)\n",
        "\n",
        "        # lambda targetの計算\n",
        "        discount_arr = cfg.discount * torch.round(imagined_done_flags)\n",
        "        lambda_target = calculate_lambda_target(imagined_rewards, discount_arr, target_values, cfg.lambda_)\n",
        "\n",
        "        # criticの損失を計算\n",
        "        critic_loss = (0.5 * (imagined_values - lambda_target.detach()) ** 2).mean()\n",
        "        critic_optimizer.zero_grad()\n",
        "        critic_loss.backward(retain_graph=True)\n",
        "        clip_grad_norm_(critic.parameters(), cfg.gradient_clipping)\n",
        "        critic_optimizer.step()\n",
        "\n",
        "        # actorの損失を計算\n",
        "        actor_loss = -(imagined_action_log_probs * (lambda_target - target_values).detach() + cfg.actor_entropy_scale * imagined_action_entropys).mean()\n",
        "\n",
        "        actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        clip_grad_norm_(actor.parameters(), cfg.gradient_clipping)\n",
        "        actor_optimizer.step()\n",
        "\n",
        "        # wandbに記録する損失関数\n",
        "        wandb.log({\"kl_loss\": kl_loss.item(),\n",
        "                \"obs_loss\": obs_loss.item(),\n",
        "                \"reward_loss\": reward_loss.item(),\n",
        "                \"discount_loss\": discount_loss.item(),\n",
        "                \"critic_loss\": critic_loss.item(),\n",
        "                \"actor_loss\": actor_loss.item()})\n",
        "\n",
        "    if (iteration + 1) % cfg.slow_critic_update == 0:\n",
        "        target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    # エピソードが終了した時に再初期化\n",
        "    if done:\n",
        "        print(f\"episode: {total_episode} total_reward: {total_reward:.8f}\")\n",
        "        print(f\"num iter: {iteration} kl loss: {kl_loss.item():.8f} obs loss: {obs_loss.item():.8f} \"\n",
        "              f\"rewrd loss: {reward_loss.item():.8f} discount loss: {discount_loss.item():.8f} \"\n",
        "              f\"critic loss: {critic_loss.item():.8f} actor loss: {actor_loss.item():.8f}\"\n",
        "        )\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        total_episode += 1\n",
        "\n",
        "        # wandb\n",
        "        wandb.log({\"episode_reward\": total_reward})\n",
        "\n",
        "        policy.reset()\n",
        "\n",
        "        # 一定エピソードごとに評価\n",
        "        if total_episode % cfg.eval_freq == 0:\n",
        "            eval_reward = evaluation(env, policy, iteration, cfg)\n",
        "            if eval_reward > best_reward:\n",
        "                best_reward = eval_reward\n",
        "                trained_models.save(\"./\")\n",
        "\n",
        "            # wandb\n",
        "            wandb.log({\"eval_reward\": eval_reward})\n",
        "\n",
        "        obs = env.reset()\n",
        "        policy.reset()\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e6d4bd8efa7544e5aa6898187267868b",
            "686eb089700142b985508f7f7d7636a0",
            "1a84f507e14e4f4c817568ff32471186",
            "460405f2498f4d66b46ab2592c7588f4",
            "c547d09a6e804c08970e74f4858dd14e",
            "324ea17a4b1b4341b84f0e543377c901",
            "9266207a1c254c7db96f5dff5999c5fd",
            "69b6c973f5d64717af052c5498b8729f",
            "cd8136ab23294ded8cf635c4b1c6a538",
            "973e4d052af44b82b05790d2c58d090b",
            "194831df6a974c8b86623abe7f4c18ca",
            "67ebe9c515b247f2a98b30583ed37406",
            "7b85a868d134402b87b8fa1e76636f89",
            "0eb9a53be4d4493687b020671cd8a830",
            "d6e02ad43d034c8b97b779b1e6eec771",
            "1283821ea98a4ac58c611866101c7d77",
            "4c8a15d7b58a45ec8beb682823d093e2",
            "3e0ee76151dc435185bc32a326cf8d54",
            "80f89c98dc4243fea2102011dd9d5bd5"
          ]
        },
        "id": "1HDhT-M1-zDs",
        "outputId": "e45b2e13-bdb9-481d-91eb-49ad34dd66e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mslhs1328\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113917711110035, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6d4bd8efa7544e5aa6898187267868b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240331_094258-0mdie9te</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/slhs1328/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C/runs/0mdie9te/workspace' target=\"_blank\">light-shadow-10</a></strong> to <a href='https://wandb.ai/slhs1328/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/slhs1328/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C' target=\"_blank\">https://wandb.ai/slhs1328/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/slhs1328/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C/runs/0mdie9te/workspace' target=\"_blank\">https://wandb.ai/slhs1328/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92_%E6%9C%80%E7%B5%82%E8%AA%B2%E9%A1%8C/runs/0mdie9te/workspace</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd8136ab23294ded8cf635c4b1c6a538"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 1 total_reward: 0.00000000\n",
            "num iter: 44 kl loss: 4.58907318 obs loss: 3868.78125000 rewrd loss: 0.91906548 discount loss: 0.37811649 critic loss: 0.00299728 actor loss: -0.10519888\n",
            "episode: 2 total_reward: 0.00000000\n",
            "num iter: 138 kl loss: 0.99502927 obs loss: 3780.68334961 rewrd loss: 0.91897970 discount loss: 0.00005927 critic loss: 0.00029442 actor loss: 0.01883146\n",
            "episode: 3 total_reward: 0.00000000\n",
            "num iter: 187 kl loss: 0.34717867 obs loss: 3771.55419922 rewrd loss: 0.91895527 discount loss: 0.00001332 critic loss: 0.00017597 actor loss: 0.00514786\n",
            "episode: 4 total_reward: 0.00000000\n",
            "num iter: 253 kl loss: 0.09160181 obs loss: 3768.23071289 rewrd loss: 0.91894650 discount loss: 0.00000959 critic loss: 0.00008528 actor loss: 0.00693887\n",
            "episode: 5 total_reward: 0.00000000\n",
            "num iter: 332 kl loss: 0.01621269 obs loss: 3767.06811523 rewrd loss: 0.91894311 discount loss: 0.00000846 critic loss: 0.00004523 actor loss: -0.00997031\n",
            "episode: 6 total_reward: 0.00000000\n",
            "num iter: 375 kl loss: 0.00832212 obs loss: 3766.87548828 rewrd loss: 0.91894209 discount loss: 0.00000827 critic loss: 0.00003947 actor loss: -0.00864615\n",
            "episode: 7 total_reward: 0.00000000\n",
            "num iter: 430 kl loss: 0.00560437 obs loss: 3766.70288086 rewrd loss: 0.91894150 discount loss: 0.00000813 critic loss: 0.00003203 actor loss: -0.00575837\n",
            "episode: 8 total_reward: 0.00000000\n",
            "num iter: 494 kl loss: 0.00473829 obs loss: 3766.64990234 rewrd loss: 0.91894072 discount loss: 0.00000792 critic loss: 0.00002953 actor loss: -0.01150593\n",
            "episode: 9 total_reward: 0.00000000\n",
            "num iter: 549 kl loss: 0.00481478 obs loss: 3766.59106445 rewrd loss: 0.91894042 discount loss: 0.00000791 critic loss: 0.00002679 actor loss: -0.01134555\n",
            "Eval(iter=549) mean: 0.0000 max: 0.0000\n",
            "episode: 10 total_reward: 0.00000000\n",
            "num iter: 592 kl loss: 0.00524840 obs loss: 3767.21508789 rewrd loss: 0.91894031 discount loss: 0.00000787 critic loss: 0.00002452 actor loss: -0.00375335\n",
            "episode: 11 total_reward: 0.00000000\n",
            "num iter: 644 kl loss: 0.00653754 obs loss: 3766.57348633 rewrd loss: 0.91894001 discount loss: 0.00000792 critic loss: 0.00002197 actor loss: -0.00643884\n",
            "episode: 12 total_reward: 0.00000000\n",
            "num iter: 686 kl loss: 0.00736144 obs loss: 3766.61474609 rewrd loss: 0.91893995 discount loss: 0.00000790 critic loss: 0.00002043 actor loss: -0.00866538\n",
            "episode: 13 total_reward: 0.00000000\n",
            "num iter: 796 kl loss: 0.01784588 obs loss: 3766.54663086 rewrd loss: 0.91893953 discount loss: 0.00000771 critic loss: 0.00001953 actor loss: -0.01113538\n",
            "episode: 14 total_reward: 0.00000000\n",
            "num iter: 837 kl loss: 0.02277322 obs loss: 3766.53222656 rewrd loss: 0.91893953 discount loss: 0.00000760 critic loss: 0.00001665 actor loss: -0.00561316\n",
            "episode: 15 total_reward: 0.00000000\n",
            "num iter: 897 kl loss: 0.03814446 obs loss: 3766.56396484 rewrd loss: 0.91893941 discount loss: 0.00000742 critic loss: 0.00001506 actor loss: -0.00860349\n",
            "episode: 16 total_reward: 0.00000000\n",
            "num iter: 957 kl loss: 0.06716272 obs loss: 3766.60986328 rewrd loss: 0.91893935 discount loss: 0.00000733 critic loss: 0.00001466 actor loss: -0.00612409\n",
            "episode: 17 total_reward: 0.00000000\n",
            "num iter: 1016 kl loss: 0.10481437 obs loss: 3767.75097656 rewrd loss: 0.91893935 discount loss: 0.00000800 critic loss: 0.00001463 actor loss: -0.00933149\n",
            "episode: 18 total_reward: 0.00000000\n",
            "num iter: 1110 kl loss: 0.30070752 obs loss: 3782.32397461 rewrd loss: 0.91893923 discount loss: 0.00001065 critic loss: 0.00001730 actor loss: -0.01178808\n",
            "episode: 19 total_reward: 0.00000000\n",
            "num iter: 1174 kl loss: 0.18260351 obs loss: 3767.99560547 rewrd loss: 0.91893935 discount loss: 0.00001200 critic loss: 0.00001215 actor loss: -0.00789448\n",
            "Eval(iter=1174) mean: 0.0000 max: 0.0000\n",
            "episode: 20 total_reward: 0.00000000\n",
            "num iter: 1243 kl loss: 0.32770079 obs loss: 3766.71582031 rewrd loss: 0.91893911 discount loss: 0.00001192 critic loss: 0.00001099 actor loss: -0.00746663\n",
            "episode: 21 total_reward: 0.00000000\n",
            "num iter: 1321 kl loss: 0.71490079 obs loss: 3766.56250000 rewrd loss: 0.91893911 discount loss: 0.00001124 critic loss: 0.00000988 actor loss: -0.00552684\n",
            "episode: 22 total_reward: 0.00000000\n",
            "num iter: 1368 kl loss: 1.14918816 obs loss: 3766.45068359 rewrd loss: 0.91893911 discount loss: 0.00001062 critic loss: 0.00000958 actor loss: -0.00613539\n",
            "episode: 23 total_reward: 0.00000000\n",
            "num iter: 1433 kl loss: 1.73203027 obs loss: 3766.43750000 rewrd loss: 0.91893905 discount loss: 0.00000959 critic loss: 0.00000922 actor loss: -0.00634352\n",
            "episode: 24 total_reward: 0.00000000\n",
            "num iter: 1506 kl loss: 1.82713294 obs loss: 3766.39355469 rewrd loss: 0.91893893 discount loss: 0.00000823 critic loss: 0.00000925 actor loss: -0.00647343\n",
            "episode: 25 total_reward: 0.00000000\n",
            "num iter: 1565 kl loss: 2.05676007 obs loss: 3766.38037109 rewrd loss: 0.91893893 discount loss: 0.00000708 critic loss: 0.00000806 actor loss: -0.00630561\n",
            "episode: 26 total_reward: 0.00000000\n",
            "num iter: 1702 kl loss: 3.00290966 obs loss: 3766.20654297 rewrd loss: 0.91893893 discount loss: 0.00000342 critic loss: 0.00000820 actor loss: -0.00832943\n",
            "episode: 27 total_reward: 0.00000000\n",
            "num iter: 1766 kl loss: 2.73123288 obs loss: 3766.19018555 rewrd loss: 0.91893893 discount loss: 0.00000311 critic loss: 0.00000709 actor loss: -0.00524012\n",
            "episode: 28 total_reward: 0.00000000\n",
            "num iter: 1806 kl loss: 2.94317770 obs loss: 3766.12573242 rewrd loss: 0.91893876 discount loss: 0.00000349 critic loss: 0.00000665 actor loss: -0.00389161\n",
            "episode: 29 total_reward: 0.00000000\n",
            "num iter: 1850 kl loss: 3.08950710 obs loss: 3766.08154297 rewrd loss: 0.91893876 discount loss: 0.00000359 critic loss: 0.00000691 actor loss: -0.00506797\n",
            "Eval(iter=1850) mean: 0.0000 max: 0.0000\n",
            "episode: 30 total_reward: 0.00000000\n",
            "num iter: 1900 kl loss: 3.86888695 obs loss: 3766.04907227 rewrd loss: 0.91893876 discount loss: 0.00000366 critic loss: 0.00000677 actor loss: -0.00500350\n",
            "episode: 31 total_reward: 0.00000000\n",
            "num iter: 1958 kl loss: 4.01557589 obs loss: 3765.96362305 rewrd loss: 0.91893876 discount loss: 0.00000442 critic loss: 0.00000693 actor loss: -0.00654278\n",
            "episode: 32 total_reward: 0.00000000\n",
            "num iter: 2042 kl loss: 4.47338152 obs loss: 3765.89428711 rewrd loss: 0.91893876 discount loss: 0.00000230 critic loss: 0.00000479 actor loss: -0.00697066\n",
            "episode: 33 total_reward: 0.00000000\n",
            "num iter: 2094 kl loss: 4.80783081 obs loss: 3765.80249023 rewrd loss: 0.91893876 discount loss: 0.00000182 critic loss: 0.00000445 actor loss: -0.00431171\n",
            "episode: 34 total_reward: 0.00000000\n",
            "num iter: 2163 kl loss: 5.10013819 obs loss: 3765.82690430 rewrd loss: 0.91893876 discount loss: 0.00000226 critic loss: 0.00000390 actor loss: -0.00498130\n",
            "episode: 35 total_reward: 0.00000000\n",
            "num iter: 2215 kl loss: 4.91373396 obs loss: 3765.70898438 rewrd loss: 0.91893876 discount loss: 0.00000203 critic loss: 0.00000370 actor loss: -0.00598402\n",
            "episode: 36 total_reward: 0.00000000\n",
            "num iter: 2278 kl loss: 4.89894438 obs loss: 3765.67236328 rewrd loss: 0.91893876 discount loss: 0.00000173 critic loss: 0.00000334 actor loss: -0.00571407\n",
            "episode: 37 total_reward: 0.00000000\n",
            "num iter: 2380 kl loss: 5.34759617 obs loss: 3765.60009766 rewrd loss: 0.91893876 discount loss: 0.00000126 critic loss: 0.00000294 actor loss: -0.00402310\n",
            "episode: 38 total_reward: 0.00000000\n",
            "num iter: 2467 kl loss: 5.75847673 obs loss: 3765.55908203 rewrd loss: 0.91893876 discount loss: 0.00000162 critic loss: 0.00000333 actor loss: -0.00572161\n",
            "episode: 39 total_reward: 0.00000000\n",
            "num iter: 2525 kl loss: 5.72836304 obs loss: 3765.52075195 rewrd loss: 0.91893876 discount loss: 0.00000147 critic loss: 0.00000277 actor loss: -0.00366982\n",
            "Eval(iter=2525) mean: 0.0000 max: 0.0000\n",
            "episode: 40 total_reward: 0.00000000\n",
            "num iter: 2604 kl loss: 5.88188887 obs loss: 3765.47509766 rewrd loss: 0.91893876 discount loss: 0.00000148 critic loss: 0.00000300 actor loss: -0.00432665\n",
            "episode: 41 total_reward: 0.00000000\n",
            "num iter: 2674 kl loss: 6.05578709 obs loss: 3765.42211914 rewrd loss: 0.91893876 discount loss: 0.00000119 critic loss: 0.00000289 actor loss: -0.00374090\n",
            "episode: 42 total_reward: 0.00000000\n",
            "num iter: 2779 kl loss: 5.72469664 obs loss: 3765.33886719 rewrd loss: 0.91893876 discount loss: 0.00000095 critic loss: 0.00000297 actor loss: -0.00461095\n",
            "episode: 43 total_reward: 0.00000000\n",
            "num iter: 2863 kl loss: 5.69067001 obs loss: 3765.30615234 rewrd loss: 0.91893876 discount loss: 0.00000120 critic loss: 0.00000281 actor loss: -0.00409239\n",
            "episode: 44 total_reward: 0.00000000\n",
            "num iter: 2925 kl loss: 5.72363997 obs loss: 3765.24560547 rewrd loss: 0.91893876 discount loss: 0.00000103 critic loss: 0.00000313 actor loss: -0.00439069\n",
            "episode: 45 total_reward: 0.00000000\n",
            "num iter: 2970 kl loss: 5.55920506 obs loss: 3765.19750977 rewrd loss: 0.91893876 discount loss: 0.00000090 critic loss: 0.00000314 actor loss: -0.00475385\n",
            "episode: 46 total_reward: 0.00000000\n",
            "num iter: 3021 kl loss: 5.37017870 obs loss: 3765.16723633 rewrd loss: 0.91893876 discount loss: 0.00000092 critic loss: 0.00000321 actor loss: -0.00424787\n",
            "episode: 47 total_reward: 0.00000000\n",
            "num iter: 3068 kl loss: 5.53894281 obs loss: 3765.13964844 rewrd loss: 0.91893876 discount loss: 0.00000075 critic loss: 0.00000314 actor loss: -0.00343886\n",
            "episode: 48 total_reward: 0.00000000\n",
            "num iter: 3204 kl loss: 5.55775023 obs loss: 3765.18896484 rewrd loss: 0.91893876 discount loss: 0.00000080 critic loss: 0.00000300 actor loss: -0.00454219\n",
            "episode: 49 total_reward: 0.00000000\n",
            "num iter: 3263 kl loss: 5.67091322 obs loss: 3765.15600586 rewrd loss: 0.91893876 discount loss: 0.00000096 critic loss: 0.00000316 actor loss: -0.00480748\n",
            "Eval(iter=3263) mean: 0.0000 max: 0.0000\n",
            "episode: 50 total_reward: 0.00000000\n",
            "num iter: 3360 kl loss: 5.40334177 obs loss: 3765.07885742 rewrd loss: 0.91893876 discount loss: 0.00000070 critic loss: 0.00000291 actor loss: -0.00320675\n",
            "episode: 51 total_reward: 0.00000000\n",
            "num iter: 3406 kl loss: 5.43489170 obs loss: 3765.10864258 rewrd loss: 0.91893876 discount loss: 0.00000066 critic loss: 0.00000320 actor loss: -0.00358398\n",
            "episode: 52 total_reward: 0.00000000\n",
            "num iter: 3470 kl loss: 5.27493286 obs loss: 3765.13598633 rewrd loss: 0.91893876 discount loss: 0.00000054 critic loss: 0.00000308 actor loss: -0.00239995\n",
            "episode: 53 total_reward: 0.00000000\n",
            "num iter: 3540 kl loss: 5.26007462 obs loss: 3765.07348633 rewrd loss: 0.91893876 discount loss: 0.00000059 critic loss: 0.00000330 actor loss: -0.00339153\n",
            "episode: 54 total_reward: 0.00000000\n",
            "num iter: 3593 kl loss: 5.24747086 obs loss: 3765.08764648 rewrd loss: 0.91893876 discount loss: 0.00000057 critic loss: 0.00000347 actor loss: -0.00443427\n",
            "episode: 55 total_reward: 0.00000000\n",
            "num iter: 3672 kl loss: 5.07290888 obs loss: 3765.05957031 rewrd loss: 0.91893876 discount loss: 0.00000061 critic loss: 0.00000317 actor loss: -0.00411069\n",
            "episode: 56 total_reward: 0.00000000\n",
            "num iter: 3735 kl loss: 5.23102379 obs loss: 3765.09350586 rewrd loss: 0.91893876 discount loss: 0.00000054 critic loss: 0.00000354 actor loss: -0.00243210\n",
            "episode: 57 total_reward: 0.00000000\n",
            "num iter: 3781 kl loss: 5.13688421 obs loss: 3764.99804688 rewrd loss: 0.91893876 discount loss: 0.00000066 critic loss: 0.00000342 actor loss: -0.00351266\n",
            "episode: 58 total_reward: 0.00000000\n",
            "num iter: 3839 kl loss: 5.05722332 obs loss: 3765.10375977 rewrd loss: 0.91893876 discount loss: 0.00000053 critic loss: 0.00000339 actor loss: -0.00333112\n",
            "episode: 59 total_reward: 0.00000000\n",
            "num iter: 3909 kl loss: 4.84544945 obs loss: 3765.01806641 rewrd loss: 0.91893876 discount loss: 0.00000057 critic loss: 0.00000368 actor loss: -0.00313037\n",
            "Eval(iter=3909) mean: 0.0000 max: 0.0000\n",
            "episode: 60 total_reward: 0.00000000\n",
            "num iter: 3984 kl loss: 4.99273825 obs loss: 3764.98657227 rewrd loss: 0.91893876 discount loss: 0.00000052 critic loss: 0.00000331 actor loss: -0.00354578\n",
            "episode: 61 total_reward: 0.00000000\n",
            "num iter: 4036 kl loss: 4.87896395 obs loss: 3764.99072266 rewrd loss: 0.91893876 discount loss: 0.00000046 critic loss: 0.00000396 actor loss: -0.00498300\n",
            "episode: 62 total_reward: 0.00000000\n",
            "num iter: 4094 kl loss: 5.07420444 obs loss: 3764.95825195 rewrd loss: 0.91893876 discount loss: 0.00000051 critic loss: 0.00000374 actor loss: -0.00361318\n",
            "episode: 63 total_reward: 0.00000000\n",
            "num iter: 4152 kl loss: 5.05117846 obs loss: 3765.31591797 rewrd loss: 0.91893876 discount loss: 0.00000043 critic loss: 0.00000384 actor loss: -0.00156376\n",
            "episode: 64 total_reward: 0.00000000\n",
            "num iter: 4239 kl loss: 4.80071068 obs loss: 3764.99267578 rewrd loss: 0.91893876 discount loss: 0.00000053 critic loss: 0.00000324 actor loss: -0.00412437\n",
            "episode: 65 total_reward: 0.00000000\n",
            "num iter: 4327 kl loss: 4.49619102 obs loss: 3764.94042969 rewrd loss: 0.91893876 discount loss: 0.00000059 critic loss: 0.00000375 actor loss: -0.00417977\n",
            "episode: 66 total_reward: 0.00000000\n",
            "num iter: 4390 kl loss: 4.46426344 obs loss: 3764.97558594 rewrd loss: 0.91893876 discount loss: 0.00000044 critic loss: 0.00000360 actor loss: -0.00446924\n",
            "episode: 67 total_reward: 0.00000000\n",
            "num iter: 4436 kl loss: 4.33981848 obs loss: 3764.95141602 rewrd loss: 0.91893876 discount loss: 0.00000046 critic loss: 0.00000411 actor loss: -0.00486406\n",
            "episode: 68 total_reward: 0.00000000\n",
            "num iter: 4527 kl loss: 4.43047619 obs loss: 3764.92407227 rewrd loss: 0.91893876 discount loss: 0.00000038 critic loss: 0.00000398 actor loss: -0.00255754\n",
            "episode: 69 total_reward: 0.00000000\n",
            "num iter: 4590 kl loss: 4.47646904 obs loss: 3764.90161133 rewrd loss: 0.91893876 discount loss: 0.00000038 critic loss: 0.00000404 actor loss: -0.00261419\n",
            "Eval(iter=4590) mean: 0.0000 max: 0.0000\n",
            "episode: 70 total_reward: 0.00000000\n",
            "num iter: 4696 kl loss: 4.35534143 obs loss: 3764.85131836 rewrd loss: 0.91893876 discount loss: 0.00000036 critic loss: 0.00000406 actor loss: -0.00487784\n",
            "episode: 71 total_reward: 0.00000000\n",
            "num iter: 4748 kl loss: 4.35208273 obs loss: 3764.97021484 rewrd loss: 0.91893876 discount loss: 0.00000034 critic loss: 0.00000451 actor loss: -0.00460443\n",
            "episode: 72 total_reward: 0.00000000\n",
            "num iter: 4801 kl loss: 4.48467684 obs loss: 3764.86889648 rewrd loss: 0.91893876 discount loss: 0.00000036 critic loss: 0.00000390 actor loss: -0.00373565\n",
            "episode: 73 total_reward: 0.00000000\n",
            "num iter: 4874 kl loss: 4.53901863 obs loss: 3764.94165039 rewrd loss: 0.91893876 discount loss: 0.00000031 critic loss: 0.00000457 actor loss: -0.00186064\n",
            "episode: 74 total_reward: 0.00000000\n",
            "num iter: 4920 kl loss: 4.41889238 obs loss: 3764.87792969 rewrd loss: 0.91893876 discount loss: 0.00000034 critic loss: 0.00000412 actor loss: -0.00346892\n",
            "episode: 75 total_reward: 0.00000000\n",
            "num iter: 5002 kl loss: 4.30373430 obs loss: 3764.87255859 rewrd loss: 0.91893876 discount loss: 0.00000033 critic loss: 0.00000423 actor loss: -0.00244256\n",
            "episode: 76 total_reward: 0.00000000\n",
            "num iter: 5106 kl loss: 4.39204931 obs loss: 3764.88159180 rewrd loss: 0.91893876 discount loss: 0.00000034 critic loss: 0.00000394 actor loss: -0.00220563\n",
            "episode: 77 total_reward: 0.00000000\n",
            "num iter: 5160 kl loss: 4.26269197 obs loss: 3764.93457031 rewrd loss: 0.91893876 discount loss: 0.00000031 critic loss: 0.00000441 actor loss: -0.00173389\n",
            "episode: 78 total_reward: 0.00000000\n",
            "num iter: 5206 kl loss: 4.35136175 obs loss: 3764.85375977 rewrd loss: 0.91893876 discount loss: 0.00000026 critic loss: 0.00000419 actor loss: -0.00426331\n",
            "episode: 79 total_reward: 0.00000000\n",
            "num iter: 5253 kl loss: 4.26881170 obs loss: 3764.88159180 rewrd loss: 0.91893876 discount loss: 0.00000028 critic loss: 0.00000409 actor loss: -0.00349118\n",
            "Eval(iter=5253) mean: 0.0000 max: 0.0000\n",
            "episode: 80 total_reward: 0.00000000\n",
            "num iter: 5317 kl loss: 4.25029564 obs loss: 3764.88500977 rewrd loss: 0.91893876 discount loss: 0.00000025 critic loss: 0.00000431 actor loss: -0.00210910\n",
            "episode: 81 total_reward: 0.00000000\n",
            "num iter: 5388 kl loss: 4.29062366 obs loss: 3764.92895508 rewrd loss: 0.91893876 discount loss: 0.00000038 critic loss: 0.00000390 actor loss: -0.00293793\n",
            "episode: 82 total_reward: 0.00000000\n",
            "num iter: 5466 kl loss: 4.27512693 obs loss: 3764.83544922 rewrd loss: 0.91893876 discount loss: 0.00000033 critic loss: 0.00000383 actor loss: -0.00267416\n",
            "episode: 83 total_reward: 0.00000000\n",
            "num iter: 5553 kl loss: 4.28317261 obs loss: 3764.83422852 rewrd loss: 0.91893876 discount loss: 0.00000027 critic loss: 0.00000367 actor loss: -0.00279796\n",
            "episode: 84 total_reward: 0.00000000\n",
            "num iter: 5666 kl loss: 4.31473827 obs loss: 3764.85058594 rewrd loss: 0.91893876 discount loss: 0.00000025 critic loss: 0.00000377 actor loss: -0.00299615\n",
            "episode: 85 total_reward: 0.00000000\n",
            "num iter: 5724 kl loss: 4.14413834 obs loss: 3764.81396484 rewrd loss: 0.91893876 discount loss: 0.00000023 critic loss: 0.00000375 actor loss: -0.00392037\n",
            "episode: 86 total_reward: 0.00000000\n",
            "num iter: 5860 kl loss: 4.25858545 obs loss: 3764.85717773 rewrd loss: 0.91893876 discount loss: 0.00000025 critic loss: 0.00000394 actor loss: -0.00326673\n",
            "episode: 87 total_reward: 0.00000000\n",
            "num iter: 5936 kl loss: 4.15641975 obs loss: 3764.88085938 rewrd loss: 0.91893876 discount loss: 0.00000027 critic loss: 0.00000415 actor loss: -0.00194954\n",
            "episode: 88 total_reward: 0.00000000\n",
            "num iter: 6016 kl loss: 4.20981026 obs loss: 3764.79418945 rewrd loss: 0.91893876 discount loss: 0.00000025 critic loss: 0.00000391 actor loss: -0.00401167\n",
            "episode: 89 total_reward: 0.00000000\n",
            "num iter: 6100 kl loss: 4.26227188 obs loss: 3764.82031250 rewrd loss: 0.91893876 discount loss: 0.00000024 critic loss: 0.00000407 actor loss: -0.00432479\n",
            "Eval(iter=6100) mean: 0.0000 max: 0.0000\n",
            "episode: 90 total_reward: 0.00000000\n",
            "num iter: 6194 kl loss: 3.93851852 obs loss: 3764.80688477 rewrd loss: 0.91893876 discount loss: 0.00000025 critic loss: 0.00000369 actor loss: -0.00251770\n",
            "episode: 91 total_reward: 0.00000000\n",
            "num iter: 6236 kl loss: 4.06161976 obs loss: 3764.85302734 rewrd loss: 0.91893876 discount loss: 0.00000022 critic loss: 0.00000384 actor loss: -0.00397127\n",
            "episode: 92 total_reward: 0.00000000\n",
            "num iter: 6299 kl loss: 3.99077058 obs loss: 3764.87109375 rewrd loss: 0.91893876 discount loss: 0.00000022 critic loss: 0.00000387 actor loss: -0.00234207\n",
            "episode: 93 total_reward: 0.00000000\n",
            "num iter: 6367 kl loss: 4.01906347 obs loss: 3764.81347656 rewrd loss: 0.91893876 discount loss: 0.00000020 critic loss: 0.00000396 actor loss: -0.00251139\n",
            "episode: 94 total_reward: 0.00000000\n",
            "num iter: 6412 kl loss: 4.08606768 obs loss: 3764.87304688 rewrd loss: 0.91893876 discount loss: 0.00000019 critic loss: 0.00000395 actor loss: -0.00313512\n",
            "episode: 95 total_reward: 0.00000000\n",
            "num iter: 6472 kl loss: 3.95070243 obs loss: 3764.81420898 rewrd loss: 0.91893876 discount loss: 0.00000021 critic loss: 0.00000420 actor loss: -0.00411679\n",
            "episode: 96 total_reward: 0.00000000\n",
            "num iter: 6512 kl loss: 3.99550843 obs loss: 3764.79174805 rewrd loss: 0.91893876 discount loss: 0.00000021 critic loss: 0.00000415 actor loss: -0.00125200\n",
            "episode: 97 total_reward: 0.00000000\n",
            "num iter: 6673 kl loss: 4.04876804 obs loss: 3764.82373047 rewrd loss: 0.91893876 discount loss: 0.00000022 critic loss: 0.00000405 actor loss: -0.00249683\n",
            "episode: 98 total_reward: 0.00000000\n",
            "num iter: 6764 kl loss: 3.99657989 obs loss: 3764.77465820 rewrd loss: 0.91893876 discount loss: 0.00000018 critic loss: 0.00000388 actor loss: -0.00194956\n",
            "episode: 99 total_reward: 0.00000000\n",
            "num iter: 6815 kl loss: 3.92462301 obs loss: 3764.76489258 rewrd loss: 0.91893876 discount loss: 0.00000020 critic loss: 0.00000433 actor loss: -0.00394727\n",
            "Eval(iter=6815) mean: 0.0000 max: 0.0000\n",
            "episode: 100 total_reward: 0.00000000\n",
            "num iter: 6905 kl loss: 3.80305934 obs loss: 3764.76782227 rewrd loss: 0.91893876 discount loss: 0.00000019 critic loss: 0.00000407 actor loss: -0.00376524\n",
            "episode: 101 total_reward: 0.00000000\n",
            "num iter: 6964 kl loss: 4.00416040 obs loss: 3764.81103516 rewrd loss: 0.91893876 discount loss: 0.00000017 critic loss: 0.00000420 actor loss: -0.00382013\n",
            "episode: 102 total_reward: 0.00000000\n",
            "num iter: 7022 kl loss: 3.95773125 obs loss: 3764.78686523 rewrd loss: 0.91893876 discount loss: 0.00000019 critic loss: 0.00000409 actor loss: -0.00250846\n",
            "episode: 103 total_reward: 0.00000000\n",
            "num iter: 7067 kl loss: 4.05497456 obs loss: 3764.74975586 rewrd loss: 0.91893876 discount loss: 0.00000015 critic loss: 0.00000445 actor loss: -0.00073951\n",
            "episode: 104 total_reward: 0.00000000\n",
            "num iter: 7143 kl loss: 4.06889200 obs loss: 3764.76806641 rewrd loss: 0.91893876 discount loss: 0.00000017 critic loss: 0.00000419 actor loss: -0.00331755\n",
            "episode: 105 total_reward: 0.00000000\n",
            "num iter: 7185 kl loss: 3.97086954 obs loss: 3764.85839844 rewrd loss: 0.91893876 discount loss: 0.00000019 critic loss: 0.00000427 actor loss: -0.00264369\n",
            "episode: 106 total_reward: 0.00000000\n",
            "num iter: 7237 kl loss: 3.85941195 obs loss: 3764.74536133 rewrd loss: 0.91893876 discount loss: 0.00000017 critic loss: 0.00000441 actor loss: -0.00169965\n",
            "episode: 107 total_reward: 0.00000000\n",
            "num iter: 7317 kl loss: 3.94418979 obs loss: 3764.76171875 rewrd loss: 0.91893876 discount loss: 0.00000019 critic loss: 0.00000401 actor loss: -0.00352948\n",
            "episode: 108 total_reward: 0.00000000\n",
            "num iter: 7374 kl loss: 3.84484100 obs loss: 3764.71826172 rewrd loss: 0.91893876 discount loss: 0.00000014 critic loss: 0.00000418 actor loss: -0.00202570\n",
            "episode: 109 total_reward: 0.00000000\n",
            "num iter: 7462 kl loss: 3.90541339 obs loss: 3764.76318359 rewrd loss: 0.91893876 discount loss: 0.00000014 critic loss: 0.00000412 actor loss: -0.00351776\n",
            "Eval(iter=7462) mean: 0.0000 max: 0.0000\n",
            "episode: 110 total_reward: 0.00000000\n",
            "num iter: 7517 kl loss: 3.98684216 obs loss: 3764.76660156 rewrd loss: 0.91893876 discount loss: 0.00000013 critic loss: 0.00000440 actor loss: -0.00128853\n",
            "episode: 111 total_reward: 0.00000000\n",
            "num iter: 7572 kl loss: 3.86217713 obs loss: 3764.97875977 rewrd loss: 0.91893876 discount loss: 0.00000017 critic loss: 0.00000405 actor loss: -0.00351477\n",
            "episode: 112 total_reward: 0.00000000\n",
            "num iter: 7642 kl loss: 3.87297606 obs loss: 3764.75756836 rewrd loss: 0.91893876 discount loss: 0.00000014 critic loss: 0.00000425 actor loss: -0.00472990\n",
            "episode: 113 total_reward: 0.00000000\n",
            "num iter: 7757 kl loss: 3.90378046 obs loss: 3764.74291992 rewrd loss: 0.91893876 discount loss: 0.00000016 critic loss: 0.00000412 actor loss: -0.00409727\n",
            "episode: 114 total_reward: 0.00000000\n",
            "num iter: 7814 kl loss: 3.85666108 obs loss: 3764.72534180 rewrd loss: 0.91893876 discount loss: 0.00000017 critic loss: 0.00000398 actor loss: -0.00159554\n",
            "episode: 115 total_reward: 0.00000000\n",
            "num iter: 7884 kl loss: 3.83020163 obs loss: 3764.79882812 rewrd loss: 0.91893876 discount loss: 0.00000014 critic loss: 0.00000525 actor loss: -0.00548775\n",
            "episode: 116 total_reward: 0.00000000\n",
            "num iter: 7945 kl loss: 3.83598375 obs loss: 3764.69311523 rewrd loss: 0.91893876 discount loss: 0.00000015 critic loss: 0.00000420 actor loss: -0.00216581\n",
            "episode: 117 total_reward: 0.00000000\n",
            "num iter: 7996 kl loss: 3.87907982 obs loss: 3764.71679688 rewrd loss: 0.91893876 discount loss: 0.00000015 critic loss: 0.00000400 actor loss: -0.00262978\n",
            "episode: 118 total_reward: 0.00000000\n",
            "num iter: 8056 kl loss: 3.80826378 obs loss: 3764.71020508 rewrd loss: 0.91893876 discount loss: 0.00000014 critic loss: 0.00000394 actor loss: -0.00310217\n",
            "episode: 119 total_reward: 0.00000000\n",
            "num iter: 8135 kl loss: 3.76288509 obs loss: 3764.75097656 rewrd loss: 0.91893876 discount loss: 0.00000015 critic loss: 0.00000386 actor loss: -0.00296332\n",
            "Eval(iter=8135) mean: 0.0000 max: 0.0000\n",
            "episode: 120 total_reward: 0.00000000\n",
            "num iter: 8171 kl loss: 3.84008622 obs loss: 3764.75830078 rewrd loss: 0.91893876 discount loss: 0.00000012 critic loss: 0.00000431 actor loss: -0.00292221\n",
            "episode: 121 total_reward: 0.00000000\n",
            "num iter: 8248 kl loss: 3.84932351 obs loss: 3764.74487305 rewrd loss: 0.91893876 discount loss: 0.00000014 critic loss: 0.00000498 actor loss: 0.00007150\n",
            "episode: 122 total_reward: 0.00000000\n",
            "num iter: 8296 kl loss: 3.74942875 obs loss: 3764.69873047 rewrd loss: 0.91893876 discount loss: 0.00000015 critic loss: 0.00000432 actor loss: -0.00218839\n",
            "episode: 123 total_reward: 0.00000000\n",
            "num iter: 8397 kl loss: 3.97138786 obs loss: 3764.74072266 rewrd loss: 0.91893876 discount loss: 0.00000015 critic loss: 0.00000410 actor loss: -0.00335694\n",
            "episode: 124 total_reward: 0.00000000\n",
            "num iter: 8464 kl loss: 3.94277573 obs loss: 3764.73388672 rewrd loss: 0.91893876 discount loss: 0.00000012 critic loss: 0.00000455 actor loss: -0.00100837\n",
            "episode: 125 total_reward: 0.00000000\n",
            "num iter: 8505 kl loss: 3.69888830 obs loss: 3764.68823242 rewrd loss: 0.91893876 discount loss: 0.00000014 critic loss: 0.00000392 actor loss: -0.00376625\n",
            "episode: 126 total_reward: 0.00000000\n",
            "num iter: 8555 kl loss: 3.80276012 obs loss: 3764.72778320 rewrd loss: 0.91893876 discount loss: 0.00000012 critic loss: 0.00000444 actor loss: -0.00349292\n",
            "episode: 127 total_reward: 0.00000000\n",
            "num iter: 8621 kl loss: 3.86826181 obs loss: 3764.69555664 rewrd loss: 0.91893876 discount loss: 0.00000011 critic loss: 0.00000440 actor loss: -0.00237076\n",
            "episode: 128 total_reward: 0.00000000\n",
            "num iter: 8723 kl loss: 3.96710706 obs loss: 3764.99389648 rewrd loss: 0.91893876 discount loss: 0.00000017 critic loss: 0.00000440 actor loss: -0.00345549\n",
            "episode: 129 total_reward: 0.00000000\n",
            "num iter: 8791 kl loss: 3.77471948 obs loss: 3764.74121094 rewrd loss: 0.91893876 discount loss: 0.00000016 critic loss: 0.00000442 actor loss: -0.00390013\n",
            "Eval(iter=8791) mean: 0.0000 max: 0.0000\n",
            "episode: 130 total_reward: 0.00000000\n",
            "num iter: 8856 kl loss: 3.76301074 obs loss: 3764.68481445 rewrd loss: 0.91893876 discount loss: 0.00000014 critic loss: 0.00000404 actor loss: -0.00346361\n",
            "episode: 131 total_reward: 0.00000000\n",
            "num iter: 8976 kl loss: 3.70785475 obs loss: 3764.68603516 rewrd loss: 0.91893876 discount loss: 0.00000011 critic loss: 0.00000427 actor loss: -0.00327912\n",
            "episode: 132 total_reward: 0.00000000\n",
            "num iter: 9026 kl loss: 3.87907720 obs loss: 3764.67504883 rewrd loss: 0.91893876 discount loss: 0.00000012 critic loss: 0.00000424 actor loss: -0.00274989\n",
            "episode: 133 total_reward: 0.00000000\n",
            "num iter: 9090 kl loss: 3.70071220 obs loss: 3764.67749023 rewrd loss: 0.91893876 discount loss: 0.00000009 critic loss: 0.00000446 actor loss: -0.00198359\n",
            "episode: 134 total_reward: 0.00000000\n",
            "num iter: 9148 kl loss: 3.67827773 obs loss: 3764.68701172 rewrd loss: 0.91893876 discount loss: 0.00000010 critic loss: 0.00000417 actor loss: -0.00229176\n",
            "episode: 135 total_reward: 0.00000000\n",
            "num iter: 9195 kl loss: 3.90422010 obs loss: 3764.69970703 rewrd loss: 0.91893876 discount loss: 0.00000011 critic loss: 0.00000473 actor loss: -0.00345378\n",
            "episode: 136 total_reward: 0.00000000\n",
            "num iter: 9268 kl loss: 3.83423376 obs loss: 3764.71313477 rewrd loss: 0.91893876 discount loss: 0.00000011 critic loss: 0.00000502 actor loss: -0.00088851\n",
            "episode: 137 total_reward: 0.00000000\n",
            "num iter: 9326 kl loss: 3.68016267 obs loss: 3764.70410156 rewrd loss: 0.91893876 discount loss: 0.00000010 critic loss: 0.00000570 actor loss: -0.00608192\n",
            "episode: 138 total_reward: 0.00000000\n",
            "num iter: 9420 kl loss: 3.63325596 obs loss: 3764.64819336 rewrd loss: 0.91893876 discount loss: 0.00000010 critic loss: 0.00000445 actor loss: -0.00419755\n",
            "episode: 139 total_reward: 0.00000000\n",
            "num iter: 9503 kl loss: 3.61228561 obs loss: 3764.67089844 rewrd loss: 0.91893876 discount loss: 0.00000010 critic loss: 0.00000506 actor loss: -0.00297383\n",
            "Eval(iter=9503) mean: 0.0000 max: 0.0000\n",
            "episode: 140 total_reward: 0.00000000\n",
            "num iter: 9556 kl loss: 3.63015676 obs loss: 3764.69458008 rewrd loss: 0.91893876 discount loss: 0.00000010 critic loss: 0.00000477 actor loss: -0.00377260\n",
            "episode: 141 total_reward: 0.00000000\n",
            "num iter: 9608 kl loss: 3.61686349 obs loss: 3764.69458008 rewrd loss: 0.91893876 discount loss: 0.00000010 critic loss: 0.00000495 actor loss: -0.00229013\n",
            "episode: 142 total_reward: 0.00000000\n",
            "num iter: 9664 kl loss: 3.59480071 obs loss: 3764.65722656 rewrd loss: 0.91893876 discount loss: 0.00000010 critic loss: 0.00000460 actor loss: -0.00391995\n",
            "episode: 143 total_reward: 0.00000000\n",
            "num iter: 9752 kl loss: 3.63397741 obs loss: 3764.67749023 rewrd loss: 0.91893876 discount loss: 0.00000009 critic loss: 0.00000576 actor loss: 0.00009802\n",
            "episode: 144 total_reward: 0.00000000\n",
            "num iter: 9797 kl loss: 3.64763069 obs loss: 3764.65307617 rewrd loss: 0.91893876 discount loss: 0.00000009 critic loss: 0.00000473 actor loss: -0.00292007\n",
            "episode: 145 total_reward: 0.00000000\n",
            "num iter: 9848 kl loss: 3.62220216 obs loss: 3764.61791992 rewrd loss: 0.91893876 discount loss: 0.00000008 critic loss: 0.00000507 actor loss: -0.00121868\n",
            "episode: 146 total_reward: 0.00000000\n",
            "num iter: 9910 kl loss: 3.44421697 obs loss: 3764.66845703 rewrd loss: 0.91893876 discount loss: 0.00000008 critic loss: 0.00000513 actor loss: -0.00267890\n",
            "episode: 147 total_reward: 0.00000000\n",
            "num iter: 9956 kl loss: 3.54130650 obs loss: 3764.63110352 rewrd loss: 0.91893876 discount loss: 0.00000008 critic loss: 0.00000501 actor loss: -0.00235911\n",
            "episode: 148 total_reward: 0.00000000\n",
            "num iter: 10029 kl loss: 3.53984737 obs loss: 3764.67431641 rewrd loss: 0.91893876 discount loss: 0.00000011 critic loss: 0.00000503 actor loss: -0.00314802\n",
            "episode: 149 total_reward: 0.00000000\n",
            "num iter: 10078 kl loss: 3.45782804 obs loss: 3764.61791992 rewrd loss: 0.91893876 discount loss: 0.00000009 critic loss: 0.00000524 actor loss: -0.00174907\n",
            "Eval(iter=10078) mean: 0.0000 max: 0.0000\n",
            "episode: 150 total_reward: 0.00000000\n",
            "num iter: 10125 kl loss: 3.58277273 obs loss: 3764.62280273 rewrd loss: 0.91893876 discount loss: 0.00000008 critic loss: 0.00000485 actor loss: -0.00132498\n",
            "episode: 151 total_reward: 0.00000000\n",
            "num iter: 10174 kl loss: 3.55914974 obs loss: 3764.67602539 rewrd loss: 0.91893876 discount loss: 0.00000008 critic loss: 0.00000489 actor loss: -0.00244531\n",
            "episode: 152 total_reward: 0.00000000\n",
            "num iter: 10234 kl loss: 3.47279692 obs loss: 3764.61303711 rewrd loss: 0.91893876 discount loss: 0.00000008 critic loss: 0.00000525 actor loss: -0.00205852\n",
            "episode: 153 total_reward: 0.00000000\n",
            "num iter: 10297 kl loss: 3.63907123 obs loss: 3764.61499023 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000523 actor loss: -0.00253895\n",
            "episode: 154 total_reward: 0.00000000\n",
            "num iter: 10346 kl loss: 3.67820597 obs loss: 3764.60083008 rewrd loss: 0.91893876 discount loss: 0.00000008 critic loss: 0.00000582 actor loss: -0.00127163\n",
            "episode: 155 total_reward: 0.00000000\n",
            "num iter: 10407 kl loss: 3.70908260 obs loss: 3764.63671875 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000553 actor loss: -0.00436638\n",
            "episode: 156 total_reward: 0.00000000\n",
            "num iter: 10461 kl loss: 3.72140336 obs loss: 3764.62695312 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000542 actor loss: -0.00342024\n",
            "episode: 157 total_reward: 0.00000000\n",
            "num iter: 10536 kl loss: 3.67163849 obs loss: 3764.62866211 rewrd loss: 0.91893876 discount loss: 0.00000009 critic loss: 0.00000501 actor loss: -0.00400897\n",
            "episode: 158 total_reward: 0.00000000\n",
            "num iter: 10605 kl loss: 3.48833251 obs loss: 3764.61376953 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000498 actor loss: -0.00243110\n",
            "episode: 159 total_reward: 0.00000000\n",
            "num iter: 10681 kl loss: 3.39477372 obs loss: 3764.63916016 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000464 actor loss: -0.00344310\n",
            "Eval(iter=10681) mean: 0.0000 max: 0.0000\n",
            "episode: 160 total_reward: 0.00000000\n",
            "num iter: 10733 kl loss: 3.58255410 obs loss: 3764.65625000 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000513 actor loss: -0.00354226\n",
            "episode: 161 total_reward: 0.00000000\n",
            "num iter: 10784 kl loss: 3.57095838 obs loss: 3764.63671875 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000531 actor loss: -0.00459144\n",
            "episode: 162 total_reward: 0.00000000\n",
            "num iter: 10858 kl loss: 3.45786572 obs loss: 3764.69873047 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000516 actor loss: -0.00282161\n",
            "episode: 163 total_reward: 0.00000000\n",
            "num iter: 10928 kl loss: 3.46394873 obs loss: 3764.64526367 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000511 actor loss: -0.00259806\n",
            "episode: 164 total_reward: 0.00000000\n",
            "num iter: 10972 kl loss: 3.59953117 obs loss: 3764.62524414 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000478 actor loss: -0.00269155\n",
            "episode: 165 total_reward: 0.00000000\n",
            "num iter: 11019 kl loss: 3.35307503 obs loss: 3764.61547852 rewrd loss: 0.91893876 discount loss: 0.00000006 critic loss: 0.00000487 actor loss: -0.00257547\n",
            "episode: 166 total_reward: 0.00000000\n",
            "num iter: 11082 kl loss: 3.25419450 obs loss: 3764.60083008 rewrd loss: 0.91893876 discount loss: 0.00000006 critic loss: 0.00000546 actor loss: -0.00219503\n",
            "episode: 167 total_reward: 0.00000000\n",
            "num iter: 11125 kl loss: 3.41752577 obs loss: 3764.58764648 rewrd loss: 0.91893876 discount loss: 0.00000006 critic loss: 0.00000638 actor loss: -0.00569219\n",
            "episode: 168 total_reward: 0.00000000\n",
            "num iter: 11199 kl loss: 3.44539833 obs loss: 3764.59130859 rewrd loss: 0.91893876 discount loss: 0.00000005 critic loss: 0.00000505 actor loss: -0.00363530\n",
            "episode: 169 total_reward: 0.00000000\n",
            "num iter: 11242 kl loss: 3.48726225 obs loss: 3764.66284180 rewrd loss: 0.91893876 discount loss: 0.00000006 critic loss: 0.00000520 actor loss: -0.00302947\n",
            "Eval(iter=11242) mean: 0.0000 max: 0.0000\n",
            "episode: 170 total_reward: 0.00000000\n",
            "num iter: 11310 kl loss: 3.55163789 obs loss: 3764.66943359 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000519 actor loss: -0.00148046\n",
            "episode: 171 total_reward: 0.00000000\n",
            "num iter: 11376 kl loss: 3.44489765 obs loss: 3764.57885742 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000492 actor loss: -0.00402948\n",
            "episode: 172 total_reward: 0.00000000\n",
            "num iter: 11438 kl loss: 3.42093897 obs loss: 3764.55761719 rewrd loss: 0.91893876 discount loss: 0.00000008 critic loss: 0.00000498 actor loss: -0.00312620\n",
            "episode: 173 total_reward: 0.00000000\n",
            "num iter: 11484 kl loss: 3.47552228 obs loss: 3764.58520508 rewrd loss: 0.91893876 discount loss: 0.00000007 critic loss: 0.00000476 actor loss: -0.00211237\n",
            "episode: 174 total_reward: 0.00000000\n",
            "num iter: 11607 kl loss: 3.37974811 obs loss: 3764.58569336 rewrd loss: 0.91893876 discount loss: 0.00000005 critic loss: 0.00000485 actor loss: -0.00276638\n",
            "episode: 175 total_reward: 0.00000000\n",
            "num iter: 11655 kl loss: 3.31756616 obs loss: 3764.58496094 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000534 actor loss: -0.00030228\n",
            "episode: 176 total_reward: 0.00000000\n",
            "num iter: 11704 kl loss: 3.30755138 obs loss: 3764.60498047 rewrd loss: 0.91893876 discount loss: 0.00000005 critic loss: 0.00000503 actor loss: -0.00110554\n",
            "episode: 177 total_reward: 0.00000000\n",
            "num iter: 11790 kl loss: 3.19449186 obs loss: 3764.59912109 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000511 actor loss: -0.00133505\n",
            "episode: 178 total_reward: 0.00000000\n",
            "num iter: 11863 kl loss: 3.46477962 obs loss: 3764.60986328 rewrd loss: 0.91893876 discount loss: 0.00000005 critic loss: 0.00000601 actor loss: -0.00041763\n",
            "episode: 179 total_reward: 0.00000000\n",
            "num iter: 11926 kl loss: 3.23642612 obs loss: 3764.59521484 rewrd loss: 0.91893876 discount loss: 0.00000005 critic loss: 0.00000571 actor loss: -0.00023210\n",
            "Eval(iter=11926) mean: 0.0000 max: 0.0000\n",
            "episode: 180 total_reward: 0.00000000\n",
            "num iter: 11978 kl loss: 3.17513180 obs loss: 3764.55273438 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000489 actor loss: -0.00260300\n",
            "episode: 181 total_reward: 0.00000000\n",
            "num iter: 12042 kl loss: 3.17517424 obs loss: 3764.54125977 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000577 actor loss: -0.00187067\n",
            "episode: 182 total_reward: 0.00000000\n",
            "num iter: 12099 kl loss: 3.45166230 obs loss: 3764.61718750 rewrd loss: 0.91893876 discount loss: 0.00000005 critic loss: 0.00000534 actor loss: -0.00008841\n",
            "episode: 183 total_reward: 0.00000000\n",
            "num iter: 12164 kl loss: 3.11520648 obs loss: 3764.59765625 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000569 actor loss: -0.00150975\n",
            "episode: 184 total_reward: 0.00000000\n",
            "num iter: 12281 kl loss: 3.19259381 obs loss: 3764.53515625 rewrd loss: 0.91893876 discount loss: 0.00000005 critic loss: 0.00000530 actor loss: -0.00247547\n",
            "episode: 185 total_reward: 0.00000000\n",
            "num iter: 12328 kl loss: 3.22553134 obs loss: 3764.55761719 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000599 actor loss: -0.00504725\n",
            "episode: 186 total_reward: 0.00000000\n",
            "num iter: 12416 kl loss: 3.40248442 obs loss: 3764.58276367 rewrd loss: 0.91893876 discount loss: 0.00000003 critic loss: 0.00000541 actor loss: -0.00370256\n",
            "episode: 187 total_reward: 0.00000000\n",
            "num iter: 12489 kl loss: 3.40018463 obs loss: 3764.69189453 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000560 actor loss: -0.00078450\n",
            "episode: 188 total_reward: 0.00000000\n",
            "num iter: 12566 kl loss: 3.16794586 obs loss: 3764.55786133 rewrd loss: 0.91893876 discount loss: 0.00000005 critic loss: 0.00000538 actor loss: -0.00106738\n",
            "episode: 189 total_reward: 0.00000000\n",
            "num iter: 12620 kl loss: 3.16269588 obs loss: 3764.55908203 rewrd loss: 0.91893876 discount loss: 0.00000005 critic loss: 0.00000553 actor loss: -0.00514713\n",
            "Eval(iter=12620) mean: 0.0000 max: 0.0000\n",
            "episode: 190 total_reward: 0.00000000\n",
            "num iter: 12717 kl loss: 3.19205713 obs loss: 3764.55786133 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000500 actor loss: -0.00277698\n",
            "episode: 191 total_reward: 0.00000000\n",
            "num iter: 12785 kl loss: 3.17264962 obs loss: 3764.56030273 rewrd loss: 0.91893876 discount loss: 0.00000003 critic loss: 0.00000539 actor loss: -0.00148098\n",
            "episode: 192 total_reward: 0.00000000\n",
            "num iter: 12872 kl loss: 3.44448066 obs loss: 3764.58325195 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000529 actor loss: -0.00176658\n",
            "episode: 193 total_reward: 0.00000000\n",
            "num iter: 12936 kl loss: 3.14563131 obs loss: 3764.56005859 rewrd loss: 0.91893876 discount loss: 0.00000003 critic loss: 0.00000490 actor loss: -0.00214340\n",
            "episode: 194 total_reward: 0.00000000\n",
            "num iter: 13005 kl loss: 3.01189780 obs loss: 3764.51586914 rewrd loss: 0.91893876 discount loss: 0.00000003 critic loss: 0.00000518 actor loss: -0.00388387\n",
            "episode: 195 total_reward: 0.00000000\n",
            "num iter: 13047 kl loss: 3.12265682 obs loss: 3764.54931641 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000539 actor loss: -0.00366188\n",
            "episode: 196 total_reward: 0.00000000\n",
            "num iter: 13091 kl loss: 3.08970523 obs loss: 3764.53710938 rewrd loss: 0.91893876 discount loss: 0.00000004 critic loss: 0.00000475 actor loss: -0.00226634\n",
            "episode: 197 total_reward: 0.00000000\n",
            "num iter: 13139 kl loss: 3.13714504 obs loss: 3764.54541016 rewrd loss: 0.91893876 discount loss: 0.00000003 critic loss: 0.00000528 actor loss: -0.00337133\n",
            "episode: 198 total_reward: 0.00000000\n",
            "num iter: 13200 kl loss: 3.12804723 obs loss: 3764.54199219 rewrd loss: 0.91893876 discount loss: 0.00000003 critic loss: 0.00000619 actor loss: -0.00528191\n",
            "episode: 199 total_reward: 0.00000000\n",
            "num iter: 13278 kl loss: 3.26468229 obs loss: 3764.54077148 rewrd loss: 0.91893876 discount loss: 0.00000003 critic loss: 0.00000519 actor loss: -0.00382359\n",
            "Eval(iter=13278) mean: 0.0000 max: 0.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-5b59115487cf>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# open-loopで予測\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimagination_horizon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_entropys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten_rnn_hiddens\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ((T-1) * B, action dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# rnn hiddenを更新, priorで次の状態を予測\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-f29dced940b3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, rnn_hidden, eval)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0maction_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 行動をサンプリングする分布: p_{\\psi} (\\hat{a}_t | \\hat{z}_t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 行動: \\hat{a}_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/one_hot_categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_categorical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mevent_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_categorical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         )\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                     raise ValueError(\n\u001b[1;32m     69\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの保存\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "trained_models.save(\"drive/MyDrive/深層強化学習/最終課題/model\")"
      ],
      "metadata": {
        "id": "wGzscEPC-0Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 環境の読み込み\n",
        "env = make_env()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 学習済みモデルの読み込み\n",
        "rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "reward_model =  RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "discount_model = DiscountModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "\n",
        "trained_models = TrainedModels(\n",
        "    rssm,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    reward_model,\n",
        "    discount_model,\n",
        "    actor,\n",
        "    critic\n",
        ")"
      ],
      "metadata": {
        "id": "2fplZNfV30Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 結果を動画で観てみるための関数\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "def display_video(frames):\n",
        "    plt.figure(figsize=(8, 8), dpi=50)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "        plt.title(\"Step %d\" % (i))\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "    display(HTML(anim.to_jshtml(default_mode='once')))\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "LBXD23pA31Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_env(seed=1234)\n",
        "\n",
        "policy = Agent(encoder, rssm, actor)\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = [obs]\n",
        "actions = []\n",
        "\n",
        "while not done:\n",
        "    action = policy(obs)\n",
        "    action_int = np.argmax(action)  # 環境に渡すときはint型\n",
        "\n",
        "    obs, reward, done, _ = env.step(action_int)\n",
        "\n",
        "    total_reward += reward\n",
        "    frames.append(obs)\n",
        "    actions.append(action_int)\n",
        "\n",
        "print('Total Reward:', total_reward)"
      ],
      "metadata": {
        "id": "L3EmaxSu3255"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_video(frames)"
      ],
      "metadata": {
        "id": "-BIVPW5x34XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat actionに対応した行動に変換する\n",
        "submission_actions = np.zeros(len(actions) * env._skip)\n",
        "for start_idx in range(env._skip):\n",
        "    submission_actions[start_idx::env._skip] = np.array(actions)\n",
        "\n",
        "np.save(\"drive/MyDrive/深層強化学習/最終課題/model\", submission_actions)"
      ],
      "metadata": {
        "id": "aSvz1xmo354V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQ8oQ-SN1QDs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}